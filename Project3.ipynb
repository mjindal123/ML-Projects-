{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Effort Table\n",
    "Mayank Jindal-100%\n",
    "Syed Omer Wajihuddin -100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ENB2012_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-2]\n",
    "YHeat = data.iloc[:,-2]\n",
    "YCool = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test and Train Split for heating and cooling load \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_H_train,Y_H_test = train_test_split(X,YHeat,test_size = 0.2,random_state = 1)\n",
    "Y_C_train,Y_C_test = train_test_split(YCool,test_size = 0.2,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regresssor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.92025293\n",
      "Test Score 0.89902278\n",
      "Accuracy of Training after Pasting: 0.92\n",
      "Accuracy of Test after Pasting: 0.90\n",
      "Accuracy of Training after Bagging: 0.92\n",
      "Accuracy of Test after Bagging: 0.90\n",
      "Accuracy of Training after ada boost: 0.92\n",
      "Accuracy of Test after ada boost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Heating load Linear regression Moddel and with ada boost, bagging and pasting ensemble \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "linreg_regr_H = LinearRegression()\n",
    "fitted = linreg_regr_H.fit(X_train,Y_H_train)\n",
    "y_pred_linreg_H = fitted.predict(X_test)\n",
    "print('Training Score {:.8f}'.format(fitted.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(fitted.score(X_test,Y_H_test)))\n",
    "rmse_linreg_H = sqrt(mean_squared_error(Y_H_test, y_pred_linreg_H))\n",
    "\n",
    "# pasting\n",
    "pas_clf_lr = BaggingRegressor(LinearRegression(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_lr.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_lr.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_lr.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_lr = BaggingRegressor(LinearRegression(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_lr.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_lr.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_lr.score(X_test, Y_H_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_lr = AdaBoostRegressor(LinearRegression(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_lr.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_lr.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_lr.score(X_test, Y_H_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.89532803\n",
      "Test Score 0.85653170\n",
      "Accuracy of Training after Pasting: 0.90\n",
      "Accuracy of Test after Pasting: 0.86\n",
      "Accuracy of Training after Bagging: 0.90\n",
      "Accuracy of Test after Bagging: 0.86\n",
      "Accuracy of Training after ada boost: 0.89\n",
      "Accuracy of test after ada boost: 0.85\n"
     ]
    }
   ],
   "source": [
    "#Cooling load Linaer regression Moddel and with ada boost ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "linreg_regr_C = LinearRegression()\n",
    "fitted = linreg_regr_C.fit(X_train,Y_C_train)\n",
    "y_pred_linreg_C = fitted.predict(X_test)\n",
    "print('Training Score {:.8f}'.format(fitted.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(fitted.score(X_test,Y_C_test)))\n",
    "rmse_linreg_H = sqrt(mean_squared_error(Y_C_test, y_pred_linreg_C))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(LinearRegression(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(LinearRegression(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(LinearRegression(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.91827732\n",
      "Test Score 0.89639346\n",
      "Best parameters{'alpha': 0.01}\n",
      "Best score 0.91802632\n",
      "Accuracy of Training after Pasting: 0.91\n",
      "Accuracy of Test after Pasting: 0.89\n",
      "Accuracy of Training after Bagging: 0.91\n",
      "Accuracy of Test after Bagging: 0.89\n",
      "Accuracy of Training after ada boost: 0.92\n",
      "Accuracy of Test after ada boost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Heating load  basic ridge regression Moddel and with ada boost, bagging and pasting ensemble \n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_regr = Ridge()\n",
    "ridge_regr.fit(X_train,Y_H_train)\n",
    "print('Training Score {:.8f}'.format(ridge_regr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(ridge_regr.score(X_test,Y_H_test)))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ridge_cv = Ridge()\n",
    "param_grid = {'alpha': [0.01,1,5,10,100,2000]}\n",
    "grid_search = GridSearchCV(ridge_cv,param_grid,cv=5)\n",
    "grid_search.fit(X_train,Y_H_train)\n",
    "              \n",
    "y_pred_ridge = grid_search.predict(X_test)\n",
    "print('Best parameters{}'.format(grid_search.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search.best_score_))  \n",
    "rmse_ridge = sqrt(mean_squared_error(Y_H_test,y_pred_ridge))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(Ridge(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(Ridge(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(Ridge(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.89249860\n",
      "Test Score 0.85435787\n",
      "Best parameters{'alpha': 1}\n",
      "Best Score 0.88912015\n",
      "Accuracy of Training after Pasting: 0.88\n",
      "Accuracy of Test after Pasting: 0.85\n",
      "Accuracy of Training after Bagging: 0.88\n",
      "Accuracy of Test after Bagging: 0.84\n",
      "Accuracy of Training after ada boost: 0.89\n",
      "Accuracy of test after ada boost: 0.85\n"
     ]
    }
   ],
   "source": [
    "#Cooling load Ridge regression Moddel and with ada boost, bagging and pasting ensemble\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_regr = Ridge(max_iter = 100000)\n",
    "lasso_regr.fit(X_train,Y_C_train)\n",
    "print('Training Score {:.8f}'.format(lasso_regr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(lasso_regr.score(X_test,Y_C_test)))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso_cv = Ridge(max_iter = 100000)\n",
    "param_grid = {'alpha':[1,5,10,20,100,2000]}\n",
    "grid_search_ls = GridSearchCV(lasso_cv,param_grid,cv = 5)\n",
    "grid_search_ls.fit(X_train,Y_C_train)  \n",
    "y_pred_lasso = grid_search_ls.predict(X_test)              \n",
    "print('Best parameters{}'.format(grid_search_ls.best_params_))\n",
    "print('Best Score {:.8f}'.format(grid_search_ls.best_score_))\n",
    "rmse_lasso = sqrt(mean_squared_error(Y_C_test,y_pred_lasso))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(Ridge(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(Ridge(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(Ridge(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.75728001\n",
      "Test Score 0.73278192\n",
      "Best parameters{'alpha': 0.01}\n",
      "Best score 0.91628202\n",
      "Accuracy of Training after Pasting: 0.76\n",
      "Accuracy of Test after Pasting: 0.74\n",
      "Accuracy of Training after Bagging: 0.76\n",
      "Accuracy of Test after Bagging: 0.73\n",
      "Accuracy of Training after ada boost: 0.82\n",
      "Accuracy of Test after ada boost: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Heating load  basic Lasso regression Moddel and with ada boost, bagging and pasting ensemble \n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_regr = Lasso()\n",
    "ridge_regr.fit(X_train,Y_H_train)\n",
    "print('Training Score {:.8f}'.format(ridge_regr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(ridge_regr.score(X_test,Y_H_test)))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ridge_cv = Lasso()\n",
    "param_grid = {'alpha': [0.01,1,5,10,100,2000]}\n",
    "grid_search = GridSearchCV(ridge_cv,param_grid,cv=5)\n",
    "grid_search.fit(X_train,Y_H_train)\n",
    "              \n",
    "y_pred_ridge = grid_search.predict(X_test)\n",
    "print('Best parameters{}'.format(grid_search.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search.best_score_))  \n",
    "rmse_ridge = sqrt(mean_squared_error(Y_H_test,y_pred_ridge))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(Lasso(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(Lasso(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(Lasso(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.76362541\n",
      "Test Score 0.73028846\n",
      "Best parameters{'alpha': 1}\n",
      "Best Score 0.76161200\n",
      "Accuracy of Training after Pasting: 0.76\n",
      "Accuracy of Test after Pasting: 0.73\n",
      "Accuracy of Training after Bagging: 0.76\n",
      "Accuracy of Test after Bagging: 0.73\n",
      "Accuracy of Training after ada boost: 0.79\n",
      "Accuracy of test after ada boost: 0.76\n"
     ]
    }
   ],
   "source": [
    "#Cooling load Lasso regression Moddel and with ada boost, bagging and pasting ensemble\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_regr = Lasso(max_iter = 100000)\n",
    "lasso_regr.fit(X_train,Y_C_train)\n",
    "print('Training Score {:.8f}'.format(lasso_regr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(lasso_regr.score(X_test,Y_C_test)))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso_cv = Lasso(max_iter = 100000)\n",
    "param_grid = {'alpha':[1,5,10,20,100,2000]}\n",
    "grid_search_ls = GridSearchCV(lasso_cv,param_grid,cv = 5)\n",
    "grid_search_ls.fit(X_train,Y_C_train)  \n",
    "y_pred_lasso = grid_search_ls.predict(X_test)              \n",
    "print('Best parameters{}'.format(grid_search_ls.best_params_))\n",
    "print('Best Score {:.8f}'.format(grid_search_ls.best_score_))\n",
    "rmse_lasso = sqrt(mean_squared_error(Y_C_test,y_pred_lasso))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(Lasso(),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(Lasso(),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(Lasso(),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c57dda1898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.94490797\n",
      "Test Score 0.91247119\n",
      "Accuracy of Training after Pasting: 0.90\n",
      "Accuracy of Test after Pasting: 0.88\n",
      "Accuracy of Training after Bagging: 0.90\n",
      "Accuracy of Test after Bagging: 0.88\n",
      "Accuracy of Training after ada boost: 0.95\n",
      "Accuracy of test after ada boost: 0.91\n"
     ]
    }
   ],
   "source": [
    "# KNN reressor For heating load with ada boost ensemble\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsRegressor(n_neighbors = k)\n",
    "    knn.fit(X_train, Y_H_train)\n",
    "    scores.append(knn.score(X_test, Y_H_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);\n",
    "plt.show()\n",
    "\n",
    "knn_regr = KNeighborsRegressor(n_neighbors = 10).fit(X_train,Y_H_train)\n",
    "y_pred_knn_H = knn_regr.predict(X_test)\n",
    "print('Training Score {:.8f}'.format(knn_regr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(knn_regr.score(X_test,Y_H_test)))\n",
    "\n",
    "# pasting\n",
    "pas_clf_dt = BaggingRegressor(KNeighborsRegressor(n_neighbors = 10),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "pas_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(pas_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(pas_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(KNeighborsRegressor(n_neighbors = 10),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# boosting\n",
    "ada_clf_dt = AdaBoostRegressor(KNeighborsRegressor(n_neighbors = 10),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c579581ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.92777868\n",
      "Test Score 0.88747484\n",
      "Accuracy of Training after Pasting: 0.88\n",
      "Accuracy of Test after Pasting: 0.85\n",
      "Accuracy of Training after Bagging: 0.88\n",
      "Accuracy of Test after Bagging: 0.84\n",
      "Accuracy of Training after ada boost: 0.94\n",
      "Accuracy of test after ada boost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# KNN regressor for cooling load with ada boost,pasting and bagging ensemble\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsRegressor(n_neighbors = k)\n",
    "    knn.fit(X_train, Y_C_train)\n",
    "    scores.append(knn.score(X_test, Y_C_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);\n",
    "plt.show()\n",
    "\n",
    "knn_regr = KNeighborsRegressor(n_neighbors = 10).fit(X_train,Y_C_train)\n",
    "y_pred_knn_C = knn_regr.predict(X_test)\n",
    "print('Training Score {:.8f}'.format(knn_regr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(knn_regr.score(X_test,Y_C_test)))\n",
    "\n",
    "# pasting\n",
    "bag_clf_dt = BaggingRegressor(KNeighborsRegressor(n_neighbors = 10),bootstrap=False, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(KNeighborsRegressor(n_neighbors = 10),bootstrap=True, n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "#boosting\n",
    "ada_clf_dt = AdaBoostRegressor(KNeighborsRegressor(n_neighbors = 10),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score from training set :0.999508504733\n",
      "R2 score from test set :0.997397269029\n",
      "Accuracy of Training after ada boost: 1.00\n",
      "Accuracy of Test after ada boost: 1.00\n"
     ]
    }
   ],
   "source": [
    "# For heating load with random forest ensemble with boosting \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_model = RandomForestRegressor(max_depth = 50)\n",
    "forest_model.fit(X_train, Y_H_train)\n",
    "fm_preds_H = forest_model.predict(X_test)\n",
    "print(\"R2 score from training set :\"+str(forest_model.score(X_train,Y_H_train)))\n",
    "print(\"R2 score from test set :\"+str(forest_model.score(X_test,Y_H_test)))\n",
    "\n",
    "#adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(RandomForestRegressor(max_depth = 50),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score from training set :0.993205614404\n",
      "R2 score from test set :0.954740615647\n",
      "Accuracy of Training after ada boost: 1.00\n",
      "Accuracy of Test after ada boost: 0.96\n"
     ]
    }
   ],
   "source": [
    "# For cooling load with random forest with boosting\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_model = RandomForestRegressor(max_depth = 50)\n",
    "forest_model.fit(X_train, Y_C_train)\n",
    "fm_preds_C = forest_model.predict(X_test)\n",
    "print(\"R2 score from training set :\"+str(forest_model.score(X_train,Y_C_train)))\n",
    "print(\"R2 score from test set :\"+str(forest_model.score(X_test,Y_C_test)))\n",
    "\n",
    "#Adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(RandomForestRegressor(max_depth = 30),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'epsilon': 0.01, 'C': 100}\n",
      "Best score 0.91541999\n",
      "Training Score 0.91762542\n",
      "Test Score 0.89837587\n",
      "Accuracy of Training after Bagging: 0.87\n",
      "Accuracy of Test after Bagging: 0.85\n",
      "Accuracy of Training after ada boost: 0.92\n",
      "Accuracy of Test after ada boost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Heating Load with linear kernel \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'linear')\n",
    "param_grid = {'C':[0.1,1,10,100],'epsilon':[0.01,0.1,1,10,]}\n",
    "grid_search_svr = GridSearchCV(svr,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_H_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_H_test)))\n",
    "\n",
    "# pasting\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'linear'),bootstrap=False,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'linear'),bootstrap=True,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "#Adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(SVR(kernel = 'linear'),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'epsilon': 1, 'C': 10}\n",
      "Best score 0.88833087\n",
      "Training Score 0.89335518\n",
      "Test Score 0.85639918\n",
      "Accuracy of Training after Pasting: 0.85\n",
      "Accuracy of Test after Pasting: 0.81\n",
      "Accuracy of Training after Bagging: 0.85\n",
      "Accuracy of Test after Bagging: 0.80\n",
      "Accuracy of Training after ada boost: 0.89\n",
      "Accuracy of Test after ada boost: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Cooling Load with linear kernel \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'linear')\n",
    "param_grid = {'C':[0.1,1,10,100],'epsilon':[0.01,0.1,1,10,]}\n",
    "grid_search_svr = GridSearchCV(svr,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_C_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_C_test)))\n",
    "\n",
    "# pasting\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'linear'),bootstrap=False,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'linear'),bootstrap=True,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "#Adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(SVR(kernel = 'linear'),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'epsilon': 1, 'C': 100}\n",
      "Best score 0.93604902\n",
      "Training Score 0.94296977\n",
      "Test Score 0.92082007\n",
      "Accuracy of Training after Pasting: 0.70\n",
      "Accuracy of Test after Pasting: 0.67\n",
      "Accuracy of Training after Bagging: 0.70\n",
      "Accuracy of Test after Bagging: 0.66\n",
      "Accuracy of Training after ada boost: 0.92\n",
      "Accuracy of Test after ada boost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Heating Load with rbf kernel \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'rbf')\n",
    "param_grid = {'C':[0.1,1,10,100],'epsilon':[0.01,0.1,1,10,]}\n",
    "grid_search_svr = GridSearchCV(svr,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_H_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_H_test)))\n",
    "\n",
    "#Pasting\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'rbf'),bootstrap=False,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'rbf'),bootstrap=True,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_H_test)))\n",
    "\n",
    "#Adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(SVR(kernel = 'rbf'),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_H_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_H_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_H_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'epsilon': 1, 'C': 100}\n",
      "Best score 0.90878680\n",
      "Training Score 0.91468694\n",
      "Test Score 0.87028836\n",
      "Accuracy of Training after Pasting: 0.71\n",
      "Accuracy of Test after Pasting: 0.65\n",
      "Accuracy of Training after Bagging: 0.70\n",
      "Accuracy of Test after Bagging: 0.64\n",
      "Accuracy of Training after ada boost: 0.89\n",
      "Accuracy of Test after ada boost: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Cooling Load with rbf kernel \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'rbf')\n",
    "param_grid = {'C':[0.1,1,10,100],'epsilon':[0.01,0.1,1,10,]}\n",
    "grid_search_svr = GridSearchCV(svr,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_C_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_C_test)))\n",
    "\n",
    "\n",
    "# pasting\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'rbf'),bootstrap=False,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Pasting: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Pasting: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "# bagging\n",
    "bag_clf_dt = BaggingRegressor(SVR(kernel = 'rbf'),bootstrap=True,n_jobs=-1, n_estimators=500, max_samples=100, random_state=1)\n",
    "bag_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after Bagging: {:.2f}'.format(bag_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after Bagging: {:.2f}'.format(bag_clf_dt.score(X_test, Y_C_test)))\n",
    "\n",
    "#Adaboost\n",
    "ada_clf_dt = AdaBoostRegressor(SVR(kernel = 'rbf'),n_estimators=200, learning_rate=0.04, random_state=1)\n",
    "ada_clf_dt.fit(X_train, Y_C_train)\n",
    "print('Accuracy of Training after ada boost: {:.2f}'.format(ada_clf_dt.score(X_train, Y_C_train)))\n",
    "print('Accuracy of Test after ada boost: {:.2f}'.format(ada_clf_dt.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble( Gradient boosting )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient Boosting for cooling and heating load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBDT regressor on training set(default paremeters): 1.00\n",
      "Accuracy of GBDT regressor on test set(default parameters): 1.00\n",
      "\n",
      "Best params {'max_depth': 10, 'learning_rate': 0.04}\n",
      "Best score 0.99708770\n",
      "Training Score 0.99969498\n",
      "Test Score 0.99635984\n",
      "Accuracy of GBDT regressor on training set(after parameters): 1.00\n",
      "Accuracy of GBDT regressor on test set(after parameters): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting for heating load\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GradientBoostingRegressor(random_state = 0)\n",
    "clf.fit(X_train, Y_H_train)\n",
    "\n",
    "print('Accuracy of GBDT regressor on training set(default paremeters): {:.2f}'\n",
    "     .format(clf.score(X_train, Y_H_train)))\n",
    "print('Accuracy of GBDT regressor on test set(default parameters): {:.2f}\\n'\n",
    "     .format(clf.score(X_test, Y_H_test)))\n",
    "\n",
    "\n",
    "param_grid = {'learning_rate':[0.01,.02,.03,.04],'max_depth':[10,20,30,40,]}\n",
    "grid_search_svr = GridSearchCV(clf,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_H_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_H_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_H_test)))\n",
    "\n",
    "clf = GradientBoostingRegressor(learning_rate = 0.04, max_depth = 10, random_state = 0)\n",
    "clf.fit(X_train, Y_H_train)\n",
    "\n",
    "print('Accuracy of GBDT regressor on training set(after parameters): {:.2f}'\n",
    "     .format(clf.score(X_train, Y_H_train)))\n",
    "print('Accuracy of GBDT regressor on test set(after parameters): {:.2f}'\n",
    "     .format(clf.score(X_test, Y_H_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBDT regressor on training set by default parameters: 0.98\n",
      "Accuracy of GBDT regressor on test set by default parameters: 0.97\n",
      "\n",
      "Best params {'max_depth': 10, 'learning_rate': 0.04}\n",
      "Best score 0.95997323\n",
      "Training Score 0.99944958\n",
      "Test Score 0.93435132\n",
      "Accuracy of GBDT regressor on training set after setting the parameters: 1.00\n",
      "Accuracy of GBDT regressor on test set after setting the parameters: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting for coling load\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "clf = GradientBoostingRegressor(random_state = 0)\n",
    "clf.fit(X_train, Y_C_train)\n",
    "\n",
    "print('Accuracy of GBDT regressor on training set by default parameters: {:.2f}'\n",
    "     .format(clf.score(X_train, Y_C_train)))\n",
    "print('Accuracy of GBDT regressor on test set by default parameters: {:.2f}\\n'\n",
    "     .format(clf.score(X_test, Y_C_test)))\n",
    "\n",
    "param_grid = {'learning_rate':[0.01,.02,.03,.04],'max_depth':[10,20,30,40,]}\n",
    "grid_search_svr = GridSearchCV(clf,param_grid,cv=10)              \n",
    "grid_search_svr.fit(X_train,Y_C_train)\n",
    "y_pred_svrlin_H = grid_search_svr.predict(X_test) \n",
    "\n",
    "print('Best params {}'.format(grid_search_svr.best_params_))\n",
    "print('Best score {:.8f}'.format(grid_search_svr.best_score_))\n",
    "\n",
    "print('Training Score {:.8f}'.format(grid_search_svr.score(X_train,Y_C_train)))\n",
    "print('Test Score {:.8f}'.format(grid_search_svr.score(X_test,Y_C_test)))\n",
    "\n",
    "\n",
    "clf = GradientBoostingRegressor(learning_rate = 0.04, max_depth = 10, random_state = 0)\n",
    "clf.fit(X_train, Y_C_train)\n",
    "\n",
    "print('Accuracy of GBDT regressor on training set after setting the parameters: {:.2f}'\n",
    "     .format(clf.score(X_train, Y_C_train)))\n",
    "print('Accuracy of GBDT regressor on test set after setting the parameters: {:.2f}'\n",
    "     .format(clf.score(X_test, Y_C_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from the results that for cooling and heating , gradient boosting is giving the accuracy of 1 by its \n",
    "default parameters. We tried to find the best parameters which are giving the maximum accuracy. learning rate was .04 and \n",
    "max_depth was 10 which is giving the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning regression for Heating Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='rmsprop' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "614/614 [==============================] - 2s 3ms/step - loss: 582.6106 - mean_squared_error: 582.6106\n",
      "Epoch 2/200\n",
      "614/614 [==============================] - 0s 179us/step - loss: 558.0841 - mean_squared_error: 558.0841\n",
      "Epoch 3/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 506.8449 - mean_squared_error: 506.8449\n",
      "Epoch 4/200\n",
      "614/614 [==============================] - 0s 195us/step - loss: 425.8636 - mean_squared_error: 425.8636\n",
      "Epoch 5/200\n",
      "614/614 [==============================] - 0s 205us/step - loss: 321.0265 - mean_squared_error: 321.0265\n",
      "Epoch 6/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 206.8624 - mean_squared_error: 206.8624\n",
      "Epoch 7/200\n",
      "614/614 [==============================] - 0s 204us/step - loss: 119.8477 - mean_squared_error: 119.8477\n",
      "Epoch 8/200\n",
      "614/614 [==============================] - 0s 217us/step - loss: 77.4007 - mean_squared_error: 77.4007\n",
      "Epoch 9/200\n",
      "614/614 [==============================] - 0s 204us/step - loss: 64.0180 - mean_squared_error: 64.0180\n",
      "Epoch 10/200\n",
      "614/614 [==============================] - 0s 208us/step - loss: 54.4526 - mean_squared_error: 54.4526\n",
      "Epoch 11/200\n",
      "614/614 [==============================] - 0s 205us/step - loss: 45.6949 - mean_squared_error: 45.6949\n",
      "Epoch 12/200\n",
      "614/614 [==============================] - 0s 196us/step - loss: 37.8339 - mean_squared_error: 37.8339\n",
      "Epoch 13/200\n",
      "614/614 [==============================] - 0s 202us/step - loss: 30.7965 - mean_squared_error: 30.7965\n",
      "Epoch 14/200\n",
      "614/614 [==============================] - 0s 206us/step - loss: 25.0251 - mean_squared_error: 25.0251\n",
      "Epoch 15/200\n",
      "614/614 [==============================] - 0s 200us/step - loss: 20.5240 - mean_squared_error: 20.5240\n",
      "Epoch 16/200\n",
      "614/614 [==============================] - 0s 205us/step - loss: 17.3875 - mean_squared_error: 17.3875\n",
      "Epoch 17/200\n",
      "614/614 [==============================] - 0s 221us/step - loss: 15.3016 - mean_squared_error: 15.3016\n",
      "Epoch 18/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 14.1441 - mean_squared_error: 14.1441\n",
      "Epoch 19/200\n",
      "614/614 [==============================] - 0s 201us/step - loss: 13.3479 - mean_squared_error: 13.3479\n",
      "Epoch 20/200\n",
      "614/614 [==============================] - 0s 193us/step - loss: 12.7859 - mean_squared_error: 12.7859\n",
      "Epoch 21/200\n",
      "614/614 [==============================] - 0s 200us/step - loss: 12.3323 - mean_squared_error: 12.3323\n",
      "Epoch 22/200\n",
      "614/614 [==============================] - 0s 197us/step - loss: 11.8976 - mean_squared_error: 11.8976\n",
      "Epoch 23/200\n",
      "614/614 [==============================] - 0s 206us/step - loss: 11.5049 - mean_squared_error: 11.5049\n",
      "Epoch 24/200\n",
      "614/614 [==============================] - 0s 207us/step - loss: 11.2543 - mean_squared_error: 11.2543\n",
      "Epoch 25/200\n",
      "614/614 [==============================] - 0s 204us/step - loss: 11.0636 - mean_squared_error: 11.0636\n",
      "Epoch 26/200\n",
      "614/614 [==============================] - 0s 188us/step - loss: 10.7922 - mean_squared_error: 10.7922\n",
      "Epoch 27/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 10.6410 - mean_squared_error: 10.6410\n",
      "Epoch 28/200\n",
      "614/614 [==============================] - 0s 210us/step - loss: 10.4593 - mean_squared_error: 10.4593\n",
      "Epoch 29/200\n",
      "614/614 [==============================] - 0s 207us/step - loss: 10.3865 - mean_squared_error: 10.3865\n",
      "Epoch 30/200\n",
      "614/614 [==============================] - 0s 199us/step - loss: 10.2860 - mean_squared_error: 10.2860\n",
      "Epoch 31/200\n",
      "614/614 [==============================] - 0s 195us/step - loss: 10.2544 - mean_squared_error: 10.2544\n",
      "Epoch 32/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 10.1410 - mean_squared_error: 10.1410\n",
      "Epoch 33/200\n",
      "614/614 [==============================] - 0s 196us/step - loss: 10.0798 - mean_squared_error: 10.0798\n",
      "Epoch 34/200\n",
      "614/614 [==============================] - 0s 198us/step - loss: 9.9725 - mean_squared_error: 9.9725\n",
      "Epoch 35/200\n",
      "614/614 [==============================] - 0s 195us/step - loss: 9.9736 - mean_squared_error: 9.9736\n",
      "Epoch 36/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 9.7908 - mean_squared_error: 9.7908\n",
      "Epoch 37/200\n",
      "614/614 [==============================] - 0s 190us/step - loss: 9.8894 - mean_squared_error: 9.8894\n",
      "Epoch 38/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 9.8439 - mean_squared_error: 9.8439\n",
      "Epoch 39/200\n",
      "614/614 [==============================] - 0s 186us/step - loss: 9.7769 - mean_squared_error: 9.7769\n",
      "Epoch 40/200\n",
      "614/614 [==============================] - 0s 181us/step - loss: 9.8753 - mean_squared_error: 9.8753\n",
      "Epoch 41/200\n",
      "614/614 [==============================] - 0s 195us/step - loss: 9.6015 - mean_squared_error: 9.6015\n",
      "Epoch 42/200\n",
      "614/614 [==============================] - 0s 163us/step - loss: 9.7877 - mean_squared_error: 9.7877\n",
      "Epoch 43/200\n",
      "614/614 [==============================] - 0s 190us/step - loss: 9.7305 - mean_squared_error: 9.7305\n",
      "Epoch 44/200\n",
      "614/614 [==============================] - 0s 193us/step - loss: 9.6546 - mean_squared_error: 9.6546\n",
      "Epoch 45/200\n",
      "614/614 [==============================] - 0s 197us/step - loss: 9.6800 - mean_squared_error: 9.6800\n",
      "Epoch 46/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 9.6238 - mean_squared_error: 9.6238\n",
      "Epoch 47/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 9.6419 - mean_squared_error: 9.6419\n",
      "Epoch 48/200\n",
      "614/614 [==============================] - 0s 189us/step - loss: 9.6426 - mean_squared_error: 9.6426\n",
      "Epoch 49/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 9.5945 - mean_squared_error: 9.5945\n",
      "Epoch 50/200\n",
      "614/614 [==============================] - 0s 185us/step - loss: 9.5027 - mean_squared_error: 9.5027\n",
      "Epoch 51/200\n",
      "614/614 [==============================] - 0s 184us/step - loss: 9.5530 - mean_squared_error: 9.5530\n",
      "Epoch 52/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 9.5422 - mean_squared_error: 9.5422\n",
      "Epoch 53/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 9.4857 - mean_squared_error: 9.4857\n",
      "Epoch 54/200\n",
      "614/614 [==============================] - 0s 166us/step - loss: 9.5391 - mean_squared_error: 9.5391\n",
      "Epoch 55/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 9.4326 - mean_squared_error: 9.4326\n",
      "Epoch 56/200\n",
      "614/614 [==============================] - 0s 184us/step - loss: 9.4783 - mean_squared_error: 9.4783\n",
      "Epoch 57/200\n",
      "614/614 [==============================] - 0s 166us/step - loss: 9.4571 - mean_squared_error: 9.4571\n",
      "Epoch 58/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.4716 - mean_squared_error: 9.4716\n",
      "Epoch 59/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.4152 - mean_squared_error: 9.4152\n",
      "Epoch 60/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 9.4810 - mean_squared_error: 9.4810\n",
      "Epoch 61/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 9.4054 - mean_squared_error: 9.4054\n",
      "Epoch 62/200\n",
      "614/614 [==============================] - 0s 164us/step - loss: 9.4013 - mean_squared_error: 9.4013\n",
      "Epoch 63/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.3322 - mean_squared_error: 9.3322\n",
      "Epoch 64/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.4116 - mean_squared_error: 9.4116\n",
      "Epoch 65/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 9.3701 - mean_squared_error: 9.3701\n",
      "Epoch 66/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 9.3095 - mean_squared_error: 9.3095\n",
      "Epoch 67/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 9.3249 - mean_squared_error: 9.3249\n",
      "Epoch 68/200\n",
      "614/614 [==============================] - 0s 177us/step - loss: 9.3263 - mean_squared_error: 9.3263\n",
      "Epoch 69/200\n",
      "614/614 [==============================] - 0s 169us/step - loss: 9.3139 - mean_squared_error: 9.3139\n",
      "Epoch 70/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.2591 - mean_squared_error: 9.2591\n",
      "Epoch 71/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 9.2588 - mean_squared_error: 9.2588\n",
      "Epoch 72/200\n",
      "614/614 [==============================] - 0s 166us/step - loss: 9.2236 - mean_squared_error: 9.2236\n",
      "Epoch 73/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 9.2602 - mean_squared_error: 9.2602\n",
      "Epoch 74/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 9.2063 - mean_squared_error: 9.2063\n",
      "Epoch 75/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 9.1865 - mean_squared_error: 9.1865\n",
      "Epoch 76/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 9.2238 - mean_squared_error: 9.2238\n",
      "Epoch 77/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 9.1915 - mean_squared_error: 9.1915\n",
      "Epoch 78/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 9.1912 - mean_squared_error: 9.1912\n",
      "Epoch 79/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 9.1330 - mean_squared_error: 9.1330\n",
      "Epoch 80/200\n",
      "614/614 [==============================] - 0s 167us/step - loss: 9.1198 - mean_squared_error: 9.1198\n",
      "Epoch 81/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 9.1028 - mean_squared_error: 9.1028\n",
      "Epoch 82/200\n",
      "614/614 [==============================] - 0s 167us/step - loss: 9.1336 - mean_squared_error: 9.1336\n",
      "Epoch 83/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 9.1138 - mean_squared_error: 9.1138\n",
      "Epoch 84/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 9.1484 - mean_squared_error: 9.1484\n",
      "Epoch 85/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 9.0887 - mean_squared_error: 9.0887\n",
      "Epoch 86/200\n",
      "614/614 [==============================] - 0s 168us/step - loss: 9.0454 - mean_squared_error: 9.0454\n",
      "Epoch 87/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 9.0450 - mean_squared_error: 9.0450\n",
      "Epoch 88/200\n",
      "614/614 [==============================] - 0s 164us/step - loss: 9.1443 - mean_squared_error: 9.1443\n",
      "Epoch 89/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 9.0545 - mean_squared_error: 9.0545\n",
      "Epoch 90/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 9.0163 - mean_squared_error: 9.0163\n",
      "Epoch 91/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 9.0565 - mean_squared_error: 9.0565\n",
      "Epoch 92/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 9.0333 - mean_squared_error: 9.0333\n",
      "Epoch 93/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 9.0081 - mean_squared_error: 9.0081\n",
      "Epoch 94/200\n",
      "614/614 [==============================] - 0s 166us/step - loss: 8.9742 - mean_squared_error: 8.9742\n",
      "Epoch 95/200\n",
      "614/614 [==============================] - 0s 167us/step - loss: 9.0258 - mean_squared_error: 9.0258\n",
      "Epoch 96/200\n",
      "614/614 [==============================] - 0s 181us/step - loss: 9.0191 - mean_squared_error: 9.0191\n",
      "Epoch 97/200\n",
      "614/614 [==============================] - 0s 169us/step - loss: 8.9957 - mean_squared_error: 8.9957\n",
      "Epoch 98/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 9.0065 - mean_squared_error: 9.0065\n",
      "Epoch 99/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 8.9561 - mean_squared_error: 8.9561\n",
      "Epoch 100/200\n",
      "614/614 [==============================] - 0s 177us/step - loss: 9.0023 - mean_squared_error: 9.0023\n",
      "Epoch 101/200\n",
      "614/614 [==============================] - 0s 167us/step - loss: 8.9066 - mean_squared_error: 8.9066\n",
      "Epoch 102/200\n",
      "614/614 [==============================] - 0s 169us/step - loss: 9.0079 - mean_squared_error: 9.0079\n",
      "Epoch 103/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.9429 - mean_squared_error: 8.9429\n",
      "Epoch 104/200\n",
      "614/614 [==============================] - 0s 162us/step - loss: 8.9561 - mean_squared_error: 8.9561\n",
      "Epoch 105/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 8.9287 - mean_squared_error: 8.9287\n",
      "Epoch 106/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 8.9511 - mean_squared_error: 8.9511\n",
      "Epoch 107/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 8.8827 - mean_squared_error: 8.8827\n",
      "Epoch 108/200\n",
      "614/614 [==============================] - 0s 167us/step - loss: 8.9151 - mean_squared_error: 8.9151\n",
      "Epoch 109/200\n",
      "614/614 [==============================] - 0s 177us/step - loss: 8.9313 - mean_squared_error: 8.9313\n",
      "Epoch 110/200\n",
      "614/614 [==============================] - 0s 182us/step - loss: 8.8344 - mean_squared_error: 8.8344\n",
      "Epoch 111/200\n",
      "614/614 [==============================] - 0s 204us/step - loss: 8.8782 - mean_squared_error: 8.8782\n",
      "Epoch 112/200\n",
      "614/614 [==============================] - 0s 196us/step - loss: 8.8501 - mean_squared_error: 8.8501\n",
      "Epoch 113/200\n",
      "614/614 [==============================] - 0s 168us/step - loss: 8.8739 - mean_squared_error: 8.8739\n",
      "Epoch 114/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.8163 - mean_squared_error: 8.8163\n",
      "Epoch 115/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.8544 - mean_squared_error: 8.8544\n",
      "Epoch 116/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 8.8166 - mean_squared_error: 8.8166\n",
      "Epoch 117/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 8.8028 - mean_squared_error: 8.8028\n",
      "Epoch 118/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.8388 - mean_squared_error: 8.8388\n",
      "Epoch 119/200\n",
      "614/614 [==============================] - 0s 177us/step - loss: 8.8386 - mean_squared_error: 8.8386\n",
      "Epoch 120/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.7686 - mean_squared_error: 8.7686\n",
      "Epoch 121/200\n",
      "614/614 [==============================] - 0s 165us/step - loss: 8.7865 - mean_squared_error: 8.7865\n",
      "Epoch 122/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 8.8723 - mean_squared_error: 8.8723\n",
      "Epoch 123/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 8.8479 - mean_squared_error: 8.8479\n",
      "Epoch 124/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.7875 - mean_squared_error: 8.7875\n",
      "Epoch 125/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 8.7793 - mean_squared_error: 8.7793\n",
      "Epoch 126/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 8.7557 - mean_squared_error: 8.7557\n",
      "Epoch 127/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.8079 - mean_squared_error: 8.8079\n",
      "Epoch 128/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.8097 - mean_squared_error: 8.8097\n",
      "Epoch 129/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7836 - mean_squared_error: 8.7836\n",
      "Epoch 130/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 8.7521 - mean_squared_error: 8.7521\n",
      "Epoch 131/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 8.7871 - mean_squared_error: 8.7871\n",
      "Epoch 132/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.7053 - mean_squared_error: 8.7053\n",
      "Epoch 133/200\n",
      "614/614 [==============================] - 0s 168us/step - loss: 8.7917 - mean_squared_error: 8.7917\n",
      "Epoch 134/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.7722 - mean_squared_error: 8.7722\n",
      "Epoch 135/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7689 - mean_squared_error: 8.7689\n",
      "Epoch 136/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 8.7533 - mean_squared_error: 8.7533\n",
      "Epoch 137/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.8159 - mean_squared_error: 8.8159\n",
      "Epoch 138/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.7182 - mean_squared_error: 8.7182\n",
      "Epoch 139/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7233 - mean_squared_error: 8.7233\n",
      "Epoch 140/200\n",
      "614/614 [==============================] - 0s 171us/step - loss: 8.7379 - mean_squared_error: 8.7379\n",
      "Epoch 141/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7375 - mean_squared_error: 8.7375\n",
      "Epoch 142/200\n",
      "614/614 [==============================] - 0s 175us/step - loss: 8.7244 - mean_squared_error: 8.7244\n",
      "Epoch 143/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 8.7046 - mean_squared_error: 8.7046\n",
      "Epoch 144/200\n",
      "614/614 [==============================] - 0s 169us/step - loss: 8.7595 - mean_squared_error: 8.7595\n",
      "Epoch 145/200\n",
      "614/614 [==============================] - 0s 179us/step - loss: 8.7061 - mean_squared_error: 8.7061\n",
      "Epoch 146/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.7421 - mean_squared_error: 8.7421\n",
      "Epoch 147/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 8.7492 - mean_squared_error: 8.7492\n",
      "Epoch 148/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 8.6727 - mean_squared_error: 8.6727\n",
      "Epoch 149/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7303 - mean_squared_error: 8.7303\n",
      "Epoch 150/200\n",
      "614/614 [==============================] - 0s 173us/step - loss: 8.6890 - mean_squared_error: 8.6890\n",
      "Epoch 151/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7101 - mean_squared_error: 8.7101\n",
      "Epoch 152/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7247 - mean_squared_error: 8.7247\n",
      "Epoch 153/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 8.6923 - mean_squared_error: 8.6923\n",
      "Epoch 154/200\n",
      "614/614 [==============================] - 0s 172us/step - loss: 8.6886 - mean_squared_error: 8.6886\n",
      "Epoch 155/200\n",
      "614/614 [==============================] - 0s 169us/step - loss: 8.6615 - mean_squared_error: 8.6615\n",
      "Epoch 156/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 8.6736 - mean_squared_error: 8.6736\n",
      "Epoch 157/200\n",
      "614/614 [==============================] - 0s 187us/step - loss: 8.6797 - mean_squared_error: 8.6797\n",
      "Epoch 158/200\n",
      "614/614 [==============================] - 0s 184us/step - loss: 8.6641 - mean_squared_error: 8.6641\n",
      "Epoch 159/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 8.6793 - mean_squared_error: 8.6793\n",
      "Epoch 160/200\n",
      "614/614 [==============================] - 0s 181us/step - loss: 8.6757 - mean_squared_error: 8.6757\n",
      "Epoch 161/200\n",
      "614/614 [==============================] - 0s 180us/step - loss: 8.7350 - mean_squared_error: 8.7350\n",
      "Epoch 162/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.7171 - mean_squared_error: 8.7171\n",
      "Epoch 163/200\n",
      "614/614 [==============================] - 0s 179us/step - loss: 8.6232 - mean_squared_error: 8.6232\n",
      "Epoch 164/200\n",
      "614/614 [==============================] - 0s 178us/step - loss: 8.6409 - mean_squared_error: 8.6409\n",
      "Epoch 165/200\n",
      "614/614 [==============================] - 0s 185us/step - loss: 8.6903 - mean_squared_error: 8.6903\n",
      "Epoch 166/200\n",
      "614/614 [==============================] - 0s 176us/step - loss: 8.6361 - mean_squared_error: 8.6361\n",
      "Epoch 167/200\n",
      "614/614 [==============================] - 0s 182us/step - loss: 8.6694 - mean_squared_error: 8.6694\n",
      "Epoch 168/200\n",
      "614/614 [==============================] - 0s 186us/step - loss: 8.6998 - mean_squared_error: 8.6998\n",
      "Epoch 169/200\n",
      "614/614 [==============================] - 0s 210us/step - loss: 8.6041 - mean_squared_error: 8.6041\n",
      "Epoch 170/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 8.6614 - mean_squared_error: 8.6614\n",
      "Epoch 171/200\n",
      "614/614 [==============================] - 0s 197us/step - loss: 8.6200 - mean_squared_error: 8.6200\n",
      "Epoch 172/200\n",
      "614/614 [==============================] - 0s 183us/step - loss: 8.6511 - mean_squared_error: 8.6511\n",
      "Epoch 173/200\n",
      "614/614 [==============================] - 0s 182us/step - loss: 8.6843 - mean_squared_error: 8.6843\n",
      "Epoch 174/200\n",
      "614/614 [==============================] - 0s 195us/step - loss: 8.6503 - mean_squared_error: 8.6503\n",
      "Epoch 175/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 8.4354 - mean_squared_error: 8.4354\n",
      "Epoch 176/200\n",
      "614/614 [==============================] - 0s 292us/step - loss: 8.7118 - mean_squared_error: 8.7118\n",
      "Epoch 177/200\n",
      "614/614 [==============================] - 0s 210us/step - loss: 8.6608 - mean_squared_error: 8.6608\n",
      "Epoch 178/200\n",
      "614/614 [==============================] - 0s 174us/step - loss: 8.6431 - mean_squared_error: 8.6431\n",
      "Epoch 179/200\n",
      "614/614 [==============================] - 0s 199us/step - loss: 8.6522 - mean_squared_error: 8.6522\n",
      "Epoch 180/200\n",
      "614/614 [==============================] - 0s 256us/step - loss: 8.6482 - mean_squared_error: 8.6482\n",
      "Epoch 181/200\n",
      "614/614 [==============================] - 0s 192us/step - loss: 8.6091 - mean_squared_error: 8.6091\n",
      "Epoch 182/200\n",
      "614/614 [==============================] - 0s 177us/step - loss: 8.6649 - mean_squared_error: 8.6649\n",
      "Epoch 183/200\n",
      "614/614 [==============================] - 0s 189us/step - loss: 8.6673 - mean_squared_error: 8.6673\n",
      "Epoch 184/200\n",
      "614/614 [==============================] - 0s 225us/step - loss: 8.6311 - mean_squared_error: 8.6311\n",
      "Epoch 185/200\n",
      "614/614 [==============================] - 0s 293us/step - loss: 8.6317 - mean_squared_error: 8.6317\n",
      "Epoch 186/200\n",
      "614/614 [==============================] - 0s 250us/step - loss: 8.6614 - mean_squared_error: 8.6614\n",
      "Epoch 187/200\n",
      "614/614 [==============================] - 0s 196us/step - loss: 8.6176 - mean_squared_error: 8.6176\n",
      "Epoch 188/200\n",
      "614/614 [==============================] - 0s 229us/step - loss: 8.6390 - mean_squared_error: 8.6390\n",
      "Epoch 189/200\n",
      "614/614 [==============================] - 0s 275us/step - loss: 8.6065 - mean_squared_error: 8.6065\n",
      "Epoch 190/200\n",
      "614/614 [==============================] - 0s 225us/step - loss: 8.6145 - mean_squared_error: 8.6145\n",
      "Epoch 191/200\n",
      "614/614 [==============================] - 0s 239us/step - loss: 8.6050 - mean_squared_error: 8.6050\n",
      "Epoch 192/200\n",
      "614/614 [==============================] - 0s 264us/step - loss: 8.6519 - mean_squared_error: 8.6519\n",
      "Epoch 193/200\n",
      "614/614 [==============================] - 0s 194us/step - loss: 8.6157 - mean_squared_error: 8.6157\n",
      "Epoch 194/200\n",
      "614/614 [==============================] - 0s 194us/step - loss: 8.6610 - mean_squared_error: 8.6610\n",
      "Epoch 195/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 8.6325 - mean_squared_error: 8.6325\n",
      "Epoch 196/200\n",
      "614/614 [==============================] - 0s 193us/step - loss: 8.6176 - mean_squared_error: 8.6176\n",
      "Epoch 197/200\n",
      "614/614 [==============================] - 0s 170us/step - loss: 8.6343 - mean_squared_error: 8.6343\n",
      "Epoch 198/200\n",
      "614/614 [==============================] - 0s 168us/step - loss: 8.5039 - mean_squared_error: 8.5039\n",
      "Epoch 199/200\n",
      "614/614 [==============================] - 0s 210us/step - loss: 8.5817 - mean_squared_error: 8.5817\n",
      "Epoch 200/200\n",
      "614/614 [==============================] - 0s 203us/step - loss: 8.6165 - mean_squared_error: 8.6165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5623d9ac8>"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "model.fit(X_train, Y_H_train, epochs = 200, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.92\n",
      "Test r2: 0.90\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(Y_H_train, y_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(Y_H_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Learning Regression for Cooling Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='rmsprop' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "614/614 [==============================] - 1s 1ms/step - loss: 679.8824 - mean_squared_error: 679.8824\n",
      "Epoch 2/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 677.3696 - mean_squared_error: 677.3696\n",
      "Epoch 3/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 673.3389 - mean_squared_error: 673.3389\n",
      "Epoch 4/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 667.3073 - mean_squared_error: 667.3073\n",
      "Epoch 5/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 658.7971 - mean_squared_error: 658.7971\n",
      "Epoch 6/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 647.2647 - mean_squared_error: 647.2647\n",
      "Epoch 7/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 632.7059 - mean_squared_error: 632.7059\n",
      "Epoch 8/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 614.2210 - mean_squared_error: 614.2210\n",
      "Epoch 9/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 592.3004 - mean_squared_error: 592.3004\n",
      "Epoch 10/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 566.1672 - mean_squared_error: 566.1672\n",
      "Epoch 11/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 536.0958 - mean_squared_error: 536.0958\n",
      "Epoch 12/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 502.4532 - mean_squared_error: 502.4532\n",
      "Epoch 13/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 465.2628 - mean_squared_error: 465.2628\n",
      "Epoch 14/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 424.9312 - mean_squared_error: 424.9312\n",
      "Epoch 15/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 381.1262 - mean_squared_error: 381.1262\n",
      "Epoch 16/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 336.1120 - mean_squared_error: 336.1120\n",
      "Epoch 17/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 290.6266 - mean_squared_error: 290.6266\n",
      "Epoch 18/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 245.3022 - mean_squared_error: 245.3022\n",
      "Epoch 19/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 202.0185 - mean_squared_error: 202.0185\n",
      "Epoch 20/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 161.8708 - mean_squared_error: 161.8708\n",
      "Epoch 21/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 128.4332 - mean_squared_error: 128.4332\n",
      "Epoch 22/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 101.4515 - mean_squared_error: 101.4515\n",
      "Epoch 23/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 82.2720 - mean_squared_error: 82.2720\n",
      "Epoch 24/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 70.8169 - mean_squared_error: 70.8169\n",
      "Epoch 25/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 65.0523 - mean_squared_error: 65.0523\n",
      "Epoch 26/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 60.6788 - mean_squared_error: 60.6788\n",
      "Epoch 27/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 56.8859 - mean_squared_error: 56.8859\n",
      "Epoch 28/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 53.3576 - mean_squared_error: 53.3576\n",
      "Epoch 29/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 49.8556 - mean_squared_error: 49.8556\n",
      "Epoch 30/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 46.5802 - mean_squared_error: 46.5802\n",
      "Epoch 31/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 43.2782 - mean_squared_error: 43.2782\n",
      "Epoch 32/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 40.2255 - mean_squared_error: 40.2255\n",
      "Epoch 33/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 37.2808 - mean_squared_error: 37.2808\n",
      "Epoch 34/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 34.4031 - mean_squared_error: 34.4031\n",
      "Epoch 35/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 31.6799 - mean_squared_error: 31.6799\n",
      "Epoch 36/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 29.2074 - mean_squared_error: 29.2074\n",
      "Epoch 37/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 26.7616 - mean_squared_error: 26.7616\n",
      "Epoch 38/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 24.5275 - mean_squared_error: 24.5275\n",
      "Epoch 39/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 22.5830 - mean_squared_error: 22.5830\n",
      "Epoch 40/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 20.8802 - mean_squared_error: 20.8802\n",
      "Epoch 41/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 19.4653 - mean_squared_error: 19.4653\n",
      "Epoch 42/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 18.1231 - mean_squared_error: 18.1231\n",
      "Epoch 43/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 16.9752 - mean_squared_error: 16.9752\n",
      "Epoch 44/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 16.0895 - mean_squared_error: 16.0895\n",
      "Epoch 45/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 15.4325 - mean_squared_error: 15.4325\n",
      "Epoch 46/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 14.8554 - mean_squared_error: 14.8554\n",
      "Epoch 47/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 14.4827 - mean_squared_error: 14.4827\n",
      "Epoch 48/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 14.1725 - mean_squared_error: 14.1725\n",
      "Epoch 49/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 13.8797 - mean_squared_error: 13.8797\n",
      "Epoch 50/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 13.6165 - mean_squared_error: 13.6165\n",
      "Epoch 51/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 13.4276 - mean_squared_error: 13.4276\n",
      "Epoch 52/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 13.2675 - mean_squared_error: 13.2675\n",
      "Epoch 53/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 13.0931 - mean_squared_error: 13.0931\n",
      "Epoch 54/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 13.0127 - mean_squared_error: 13.0127\n",
      "Epoch 55/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.8957 - mean_squared_error: 12.8957\n",
      "Epoch 56/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.7654 - mean_squared_error: 12.7654\n",
      "Epoch 57/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.6779 - mean_squared_error: 12.6779\n",
      "Epoch 58/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.5990 - mean_squared_error: 12.5990\n",
      "Epoch 59/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.5128 - mean_squared_error: 12.5128\n",
      "Epoch 60/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 12.4283 - mean_squared_error: 12.4283\n",
      "Epoch 61/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.3174 - mean_squared_error: 12.3174\n",
      "Epoch 62/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 12.2884 - mean_squared_error: 12.2884\n",
      "Epoch 63/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.2143 - mean_squared_error: 12.2143\n",
      "Epoch 64/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 12.1755 - mean_squared_error: 12.1755\n",
      "Epoch 65/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.1067 - mean_squared_error: 12.1067\n",
      "Epoch 66/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.0441 - mean_squared_error: 12.0441\n",
      "Epoch 67/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 12.0061 - mean_squared_error: 12.0061\n",
      "Epoch 68/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.9797 - mean_squared_error: 11.9797\n",
      "Epoch 69/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.9027 - mean_squared_error: 11.9027\n",
      "Epoch 70/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.8715 - mean_squared_error: 11.8715\n",
      "Epoch 71/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.7813 - mean_squared_error: 11.7813\n",
      "Epoch 72/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.8141 - mean_squared_error: 11.8141\n",
      "Epoch 73/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.7987 - mean_squared_error: 11.7987\n",
      "Epoch 74/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.6927 - mean_squared_error: 11.6927\n",
      "Epoch 75/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.7263 - mean_squared_error: 11.7263\n",
      "Epoch 76/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.7135 - mean_squared_error: 11.7135\n",
      "Epoch 77/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.6229 - mean_squared_error: 11.6229\n",
      "Epoch 78/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.6189 - mean_squared_error: 11.6189\n",
      "Epoch 79/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.6250 - mean_squared_error: 11.6250\n",
      "Epoch 80/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.5984 - mean_squared_error: 11.5984\n",
      "Epoch 81/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.5896 - mean_squared_error: 11.5896\n",
      "Epoch 82/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.5573 - mean_squared_error: 11.5573\n",
      "Epoch 83/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.5488 - mean_squared_error: 11.5488\n",
      "Epoch 84/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4828 - mean_squared_error: 11.4828\n",
      "Epoch 85/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.5091 - mean_squared_error: 11.5091\n",
      "Epoch 86/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4697 - mean_squared_error: 11.4697\n",
      "Epoch 87/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4589 - mean_squared_error: 11.4589\n",
      "Epoch 88/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4252 - mean_squared_error: 11.4252\n",
      "Epoch 89/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.4302 - mean_squared_error: 11.4302\n",
      "Epoch 90/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4364 - mean_squared_error: 11.4364\n",
      "Epoch 91/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.4023 - mean_squared_error: 11.4023\n",
      "Epoch 92/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 11.3793 - mean_squared_error: 11.3793\n",
      "Epoch 93/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 11.4200 - mean_squared_error: 11.4200\n",
      "Epoch 94/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.3844 - mean_squared_error: 11.3844\n",
      "Epoch 95/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.3528 - mean_squared_error: 11.3528\n",
      "Epoch 96/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.3496 - mean_squared_error: 11.3496\n",
      "Epoch 97/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 11.3382 - mean_squared_error: 11.3382\n",
      "Epoch 98/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2892 - mean_squared_error: 11.2892\n",
      "Epoch 99/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.3000 - mean_squared_error: 11.3000\n",
      "Epoch 100/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.3120 - mean_squared_error: 11.3120\n",
      "Epoch 101/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2872 - mean_squared_error: 11.2872\n",
      "Epoch 102/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2874 - mean_squared_error: 11.2874\n",
      "Epoch 103/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2637 - mean_squared_error: 11.2637\n",
      "Epoch 104/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2685 - mean_squared_error: 11.2685\n",
      "Epoch 105/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2727 - mean_squared_error: 11.2727\n",
      "Epoch 106/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 11.2454 - mean_squared_error: 11.2454\n",
      "Epoch 107/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2491 - mean_squared_error: 11.2491\n",
      "Epoch 108/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2066 - mean_squared_error: 11.2066\n",
      "Epoch 109/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.2042 - mean_squared_error: 11.2042\n",
      "Epoch 110/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1835 - mean_squared_error: 11.1835\n",
      "Epoch 111/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1966 - mean_squared_error: 11.1966\n",
      "Epoch 112/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1562 - mean_squared_error: 11.1562\n",
      "Epoch 113/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1740 - mean_squared_error: 11.1740\n",
      "Epoch 114/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1420 - mean_squared_error: 11.1420\n",
      "Epoch 115/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1337 - mean_squared_error: 11.1337\n",
      "Epoch 116/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1450 - mean_squared_error: 11.1450\n",
      "Epoch 117/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0995 - mean_squared_error: 11.0995\n",
      "Epoch 118/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1351 - mean_squared_error: 11.1351\n",
      "Epoch 119/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1023 - mean_squared_error: 11.1023\n",
      "Epoch 120/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.1100 - mean_squared_error: 11.1100\n",
      "Epoch 121/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0758 - mean_squared_error: 11.0758\n",
      "Epoch 122/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0771 - mean_squared_error: 11.0771\n",
      "Epoch 123/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0867 - mean_squared_error: 11.0867\n",
      "Epoch 124/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0572 - mean_squared_error: 11.0572\n",
      "Epoch 125/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0529 - mean_squared_error: 11.0529\n",
      "Epoch 126/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0169 - mean_squared_error: 11.0169\n",
      "Epoch 127/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0270 - mean_squared_error: 11.0270\n",
      "Epoch 128/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9877 - mean_squared_error: 10.9877\n",
      "Epoch 129/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0216 - mean_squared_error: 11.0216\n",
      "Epoch 130/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 11.0157 - mean_squared_error: 11.0157\n",
      "Epoch 131/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9834 - mean_squared_error: 10.9834\n",
      "Epoch 132/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 11.0004 - mean_squared_error: 11.0004\n",
      "Epoch 133/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9888 - mean_squared_error: 10.9888\n",
      "Epoch 134/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9641 - mean_squared_error: 10.9641\n",
      "Epoch 135/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9614 - mean_squared_error: 10.9614\n",
      "Epoch 136/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9726 - mean_squared_error: 10.9726\n",
      "Epoch 137/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9868 - mean_squared_error: 10.9868\n",
      "Epoch 138/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9269 - mean_squared_error: 10.9269\n",
      "Epoch 139/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9306 - mean_squared_error: 10.9306\n",
      "Epoch 140/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9476 - mean_squared_error: 10.9476\n",
      "Epoch 141/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8871 - mean_squared_error: 10.8871\n",
      "Epoch 142/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8050 - mean_squared_error: 10.8050\n",
      "Epoch 143/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.9350 - mean_squared_error: 10.9350\n",
      "Epoch 144/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.8975 - mean_squared_error: 10.8975\n",
      "Epoch 145/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8396 - mean_squared_error: 10.8396\n",
      "Epoch 146/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.8674 - mean_squared_error: 10.8674\n",
      "Epoch 147/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8264 - mean_squared_error: 10.8264\n",
      "Epoch 148/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8911 - mean_squared_error: 10.8911\n",
      "Epoch 149/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.8797 - mean_squared_error: 10.8797\n",
      "Epoch 150/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8448 - mean_squared_error: 10.8448\n",
      "Epoch 151/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8220 - mean_squared_error: 10.8220\n",
      "Epoch 152/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8497 - mean_squared_error: 10.8497\n",
      "Epoch 153/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8049 - mean_squared_error: 10.8049\n",
      "Epoch 154/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.8344 - mean_squared_error: 10.8344\n",
      "Epoch 155/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.8157 - mean_squared_error: 10.8157\n",
      "Epoch 156/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.8222 - mean_squared_error: 10.8222\n",
      "Epoch 157/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.7806 - mean_squared_error: 10.7806\n",
      "Epoch 158/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.7858 - mean_squared_error: 10.7858\n",
      "Epoch 159/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7734 - mean_squared_error: 10.7734\n",
      "Epoch 160/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.7712 - mean_squared_error: 10.7712\n",
      "Epoch 161/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7768 - mean_squared_error: 10.7768\n",
      "Epoch 162/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.7636 - mean_squared_error: 10.7636\n",
      "Epoch 163/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7495 - mean_squared_error: 10.7495\n",
      "Epoch 164/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7320 - mean_squared_error: 10.7320\n",
      "Epoch 165/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.7361 - mean_squared_error: 10.7361\n",
      "Epoch 166/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.7221 - mean_squared_error: 10.7221\n",
      "Epoch 167/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7066 - mean_squared_error: 10.7066\n",
      "Epoch 168/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.7188 - mean_squared_error: 10.7188\n",
      "Epoch 169/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.6945 - mean_squared_error: 10.6945\n",
      "Epoch 170/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.6913 - mean_squared_error: 10.6913\n",
      "Epoch 171/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.6865 - mean_squared_error: 10.6865\n",
      "Epoch 172/300\n",
      "614/614 [==============================] - 0s 85us/step - loss: 10.7047 - mean_squared_error: 10.7047\n",
      "Epoch 173/300\n",
      "614/614 [==============================] - 0s 72us/step - loss: 10.6813 - mean_squared_error: 10.6813\n",
      "Epoch 174/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6895 - mean_squared_error: 10.6895\n",
      "Epoch 175/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.6797 - mean_squared_error: 10.6797\n",
      "Epoch 176/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.6059 - mean_squared_error: 10.6059\n",
      "Epoch 177/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6280 - mean_squared_error: 10.6280\n",
      "Epoch 178/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6650 - mean_squared_error: 10.6650\n",
      "Epoch 179/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.6391 - mean_squared_error: 10.6391\n",
      "Epoch 180/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6471 - mean_squared_error: 10.6471\n",
      "Epoch 181/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6443 - mean_squared_error: 10.6443\n",
      "Epoch 182/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6332 - mean_squared_error: 10.6332\n",
      "Epoch 183/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6054 - mean_squared_error: 10.6054\n",
      "Epoch 184/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 10.5992 - mean_squared_error: 10.5992\n",
      "Epoch 185/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6110 - mean_squared_error: 10.6110\n",
      "Epoch 186/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 10.5639 - mean_squared_error: 10.5639\n",
      "Epoch 187/300\n",
      "614/614 [==============================] - 0s 65us/step - loss: 10.5683 - mean_squared_error: 10.5683\n",
      "Epoch 188/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.6267 - mean_squared_error: 10.6267\n",
      "Epoch 189/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5817 - mean_squared_error: 10.5817\n",
      "Epoch 190/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5817 - mean_squared_error: 10.5817\n",
      "Epoch 191/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5727 - mean_squared_error: 10.5727\n",
      "Epoch 192/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5726 - mean_squared_error: 10.5726\n",
      "Epoch 193/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5699 - mean_squared_error: 10.5699\n",
      "Epoch 194/300\n",
      "614/614 [==============================] - 0s 59us/step - loss: 10.5836 - mean_squared_error: 10.5836\n",
      "Epoch 195/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.5550 - mean_squared_error: 10.5550\n",
      "Epoch 196/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.5518 - mean_squared_error: 10.5518\n",
      "Epoch 197/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.5259 - mean_squared_error: 10.5259\n",
      "Epoch 198/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.5235 - mean_squared_error: 10.5235\n",
      "Epoch 199/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4978 - mean_squared_error: 10.4978\n",
      "Epoch 200/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.5123 - mean_squared_error: 10.5123\n",
      "Epoch 201/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.5329 - mean_squared_error: 10.5329\n",
      "Epoch 202/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4708 - mean_squared_error: 10.4708\n",
      "Epoch 203/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.5340 - mean_squared_error: 10.5340\n",
      "Epoch 204/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4927 - mean_squared_error: 10.4927\n",
      "Epoch 205/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.5153 - mean_squared_error: 10.5153\n",
      "Epoch 206/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4712 - mean_squared_error: 10.4712\n",
      "Epoch 207/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.4670 - mean_squared_error: 10.4670\n",
      "Epoch 208/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4944 - mean_squared_error: 10.4944\n",
      "Epoch 209/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4470 - mean_squared_error: 10.4470\n",
      "Epoch 210/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4326 - mean_squared_error: 10.4326\n",
      "Epoch 211/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4715 - mean_squared_error: 10.4715\n",
      "Epoch 212/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4649 - mean_squared_error: 10.4649\n",
      "Epoch 213/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4351 - mean_squared_error: 10.4351\n",
      "Epoch 214/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4370 - mean_squared_error: 10.4370\n",
      "Epoch 215/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.4473 - mean_squared_error: 10.4473\n",
      "Epoch 216/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4020 - mean_squared_error: 10.4020\n",
      "Epoch 217/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4700 - mean_squared_error: 10.4700\n",
      "Epoch 218/300\n",
      "614/614 [==============================] - 0s 52us/step - loss: 10.4035 - mean_squared_error: 10.4035\n",
      "Epoch 219/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3746 - mean_squared_error: 10.3746\n",
      "Epoch 220/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4516 - mean_squared_error: 10.4516\n",
      "Epoch 221/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.4223 - mean_squared_error: 10.4223\n",
      "Epoch 222/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3782 - mean_squared_error: 10.3782\n",
      "Epoch 223/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.4130 - mean_squared_error: 10.4130\n",
      "Epoch 224/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4092 - mean_squared_error: 10.4092\n",
      "Epoch 225/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.4187 - mean_squared_error: 10.4187\n",
      "Epoch 226/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3938 - mean_squared_error: 10.3938\n",
      "Epoch 227/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3894 - mean_squared_error: 10.3894\n",
      "Epoch 228/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3605 - mean_squared_error: 10.3605\n",
      "Epoch 229/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3909 - mean_squared_error: 10.3909\n",
      "Epoch 230/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3758 - mean_squared_error: 10.3758\n",
      "Epoch 231/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3400 - mean_squared_error: 10.3400\n",
      "Epoch 232/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3431 - mean_squared_error: 10.3431\n",
      "Epoch 233/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3490 - mean_squared_error: 10.3490\n",
      "Epoch 234/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3904 - mean_squared_error: 10.3904\n",
      "Epoch 235/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3475 - mean_squared_error: 10.3475\n",
      "Epoch 236/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3461 - mean_squared_error: 10.3461\n",
      "Epoch 237/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3334 - mean_squared_error: 10.3334\n",
      "Epoch 238/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3449 - mean_squared_error: 10.3449\n",
      "Epoch 239/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3249 - mean_squared_error: 10.3249\n",
      "Epoch 240/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3268 - mean_squared_error: 10.3268\n",
      "Epoch 241/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3483 - mean_squared_error: 10.3483\n",
      "Epoch 242/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3220 - mean_squared_error: 10.3220\n",
      "Epoch 243/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3339 - mean_squared_error: 10.3339\n",
      "Epoch 244/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3174 - mean_squared_error: 10.3174\n",
      "Epoch 245/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.3269 - mean_squared_error: 10.3269\n",
      "Epoch 246/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3179 - mean_squared_error: 10.3179\n",
      "Epoch 247/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2838 - mean_squared_error: 10.2838\n",
      "Epoch 248/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3091 - mean_squared_error: 10.3091\n",
      "Epoch 249/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.3079 - mean_squared_error: 10.3079\n",
      "Epoch 250/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2981 - mean_squared_error: 10.2981\n",
      "Epoch 251/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2976 - mean_squared_error: 10.2976\n",
      "Epoch 252/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2851 - mean_squared_error: 10.2851\n",
      "Epoch 253/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2827 - mean_squared_error: 10.2827\n",
      "Epoch 254/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2340 - mean_squared_error: 10.2340\n",
      "Epoch 255/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2695 - mean_squared_error: 10.2695\n",
      "Epoch 256/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2461 - mean_squared_error: 10.2461\n",
      "Epoch 257/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2368 - mean_squared_error: 10.2368\n",
      "Epoch 258/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2520 - mean_squared_error: 10.2520\n",
      "Epoch 259/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2250 - mean_squared_error: 10.2250\n",
      "Epoch 260/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2495 - mean_squared_error: 10.2495\n",
      "Epoch 261/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2542 - mean_squared_error: 10.2542\n",
      "Epoch 262/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2400 - mean_squared_error: 10.2400\n",
      "Epoch 263/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2465 - mean_squared_error: 10.2465\n",
      "Epoch 264/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2677 - mean_squared_error: 10.2677\n",
      "Epoch 265/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2434 - mean_squared_error: 10.2434\n",
      "Epoch 266/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2227 - mean_squared_error: 10.2227\n",
      "Epoch 267/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2541 - mean_squared_error: 10.2541\n",
      "Epoch 268/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1960 - mean_squared_error: 10.1960\n",
      "Epoch 269/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2428 - mean_squared_error: 10.2428\n",
      "Epoch 270/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2124 - mean_squared_error: 10.2124\n",
      "Epoch 271/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2209 - mean_squared_error: 10.2209\n",
      "Epoch 272/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1762 - mean_squared_error: 10.1762\n",
      "Epoch 273/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1997 - mean_squared_error: 10.1997\n",
      "Epoch 274/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2074 - mean_squared_error: 10.2074\n",
      "Epoch 275/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2264 - mean_squared_error: 10.2264\n",
      "Epoch 276/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1968 - mean_squared_error: 10.1968\n",
      "Epoch 277/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2037 - mean_squared_error: 10.2037\n",
      "Epoch 278/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.2101 - mean_squared_error: 10.2101\n",
      "Epoch 279/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1879 - mean_squared_error: 10.1879\n",
      "Epoch 280/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.2256 - mean_squared_error: 10.2256\n",
      "Epoch 281/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1913 - mean_squared_error: 10.1913\n",
      "Epoch 282/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1817 - mean_squared_error: 10.1817\n",
      "Epoch 283/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1820 - mean_squared_error: 10.1820\n",
      "Epoch 284/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1743 - mean_squared_error: 10.1743\n",
      "Epoch 285/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1974 - mean_squared_error: 10.1974\n",
      "Epoch 286/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1857 - mean_squared_error: 10.1857\n",
      "Epoch 287/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1971 - mean_squared_error: 10.1971\n",
      "Epoch 288/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1705 - mean_squared_error: 10.1705\n",
      "Epoch 289/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1877 - mean_squared_error: 10.1877\n",
      "Epoch 290/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1245 - mean_squared_error: 10.1245\n",
      "Epoch 291/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1728 - mean_squared_error: 10.1728\n",
      "Epoch 292/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1436 - mean_squared_error: 10.1436\n",
      "Epoch 293/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1550 - mean_squared_error: 10.1550\n",
      "Epoch 294/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1557 - mean_squared_error: 10.1557\n",
      "Epoch 295/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1523 - mean_squared_error: 10.1523\n",
      "Epoch 296/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1421 - mean_squared_error: 10.1421\n",
      "Epoch 297/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1494 - mean_squared_error: 10.1494\n",
      "Epoch 298/300\n",
      "614/614 [==============================] - 0s 46us/step - loss: 10.1354 - mean_squared_error: 10.1354\n",
      "Epoch 299/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1461 - mean_squared_error: 10.1461\n",
      "Epoch 300/300\n",
      "614/614 [==============================] - 0s 39us/step - loss: 10.1287 - mean_squared_error: 10.1287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c570ecd0f0>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "model.fit(X_train, Y_C_train, epochs = 200, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.81\n",
      "Test r2: 0.79\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(Y_C_train, y_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(Y_C_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally after running various regression modles like linear,KNN,SVR,RandonForest,ridge,lasso and Deep Learning we found that random forest was giving the maximum accuracy of 0.99.Bagging is by default applied in the RandomForest model. After applying ada boosting(ensemble) on RandomForest,accuracy becacme 1(100%) which is as same as simply applying the gradient boosting(ensemble).Ridge, lasso and deep learning  was not giving good accuracy after applying boosting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification for overall load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining the Heat and Cooling load into Total load\n",
    "Y=YHeat+YCool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFkCAYAAABIPLOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCRJxf4vQiv0G2jodbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c503781ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyzing the Load distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see from the plot above that most load lies in 20-30 range which is of low efficient category.Average and high \n",
    "# efficient load almost have same distribution.\n",
    "\n",
    "#categorize the model label as 1 being low efficient ,2 being Average efficient  and 3 being High efficient.\n",
    "\n",
    "for i in range(0,768):\n",
    "    if (Y[i]>=16.95 and Y[i]<=40):\n",
    "        Y[i]=1\n",
    "    elif (Y[i]>=40.01 and Y[i]<=58):\n",
    "        Y[i]=2\n",
    "    else:\n",
    "        Y[i]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test and Train Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y_train,Y_test = train_test_split(dummy_y,test_size = 0.2,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=8, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "614/614 [==============================] - 2s 3ms/step - loss: 1.0845 - acc: 0.2883\n",
      "Epoch 2/450\n",
      "614/614 [==============================] - 0s 219us/step - loss: 0.9410 - acc: 0.8274\n",
      "Epoch 3/450\n",
      "614/614 [==============================] - 0s 221us/step - loss: 0.7741 - acc: 0.8453\n",
      "Epoch 4/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.6072 - acc: 0.8453\n",
      "Epoch 5/450\n",
      "614/614 [==============================] - 0s 220us/step - loss: 0.4841 - acc: 0.8453\n",
      "Epoch 6/450\n",
      "614/614 [==============================] - 0s 214us/step - loss: 0.4209 - acc: 0.8453\n",
      "Epoch 7/450\n",
      "614/614 [==============================] - 0s 224us/step - loss: 0.3985 - acc: 0.8453\n",
      "Epoch 8/450\n",
      "614/614 [==============================] - 0s 224us/step - loss: 0.3905 - acc: 0.8453\n",
      "Epoch 9/450\n",
      "614/614 [==============================] - 0s 222us/step - loss: 0.3835 - acc: 0.8453\n",
      "Epoch 10/450\n",
      "614/614 [==============================] - 0s 209us/step - loss: 0.3769 - acc: 0.8453\n",
      "Epoch 11/450\n",
      "614/614 [==============================] - 0s 220us/step - loss: 0.3703 - acc: 0.8534\n",
      "Epoch 12/450\n",
      "614/614 [==============================] - 0s 221us/step - loss: 0.3626 - acc: 0.8583\n",
      "Epoch 13/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.3555 - acc: 0.8616\n",
      "Epoch 14/450\n",
      "614/614 [==============================] - 0s 227us/step - loss: 0.3478 - acc: 0.8648\n",
      "Epoch 15/450\n",
      "614/614 [==============================] - 0s 222us/step - loss: 0.3417 - acc: 0.8713\n",
      "Epoch 16/450\n",
      "614/614 [==============================] - 0s 226us/step - loss: 0.3358 - acc: 0.8697\n",
      "Epoch 17/450\n",
      "614/614 [==============================] - 0s 224us/step - loss: 0.3303 - acc: 0.8795\n",
      "Epoch 18/450\n",
      "614/614 [==============================] - 0s 217us/step - loss: 0.3247 - acc: 0.8762\n",
      "Epoch 19/450\n",
      "614/614 [==============================] - 0s 224us/step - loss: 0.3190 - acc: 0.8860\n",
      "Epoch 20/450\n",
      "614/614 [==============================] - 0s 229us/step - loss: 0.3133 - acc: 0.8860\n",
      "Epoch 21/450\n",
      "614/614 [==============================] - 0s 231us/step - loss: 0.3083 - acc: 0.8876\n",
      "Epoch 22/450\n",
      "614/614 [==============================] - 0s 226us/step - loss: 0.3036 - acc: 0.8860\n",
      "Epoch 23/450\n",
      "614/614 [==============================] - 0s 218us/step - loss: 0.2997 - acc: 0.8811\n",
      "Epoch 24/450\n",
      "614/614 [==============================] - 0s 225us/step - loss: 0.2947 - acc: 0.8893\n",
      "Epoch 25/450\n",
      "614/614 [==============================] - 0s 226us/step - loss: 0.2932 - acc: 0.8893\n",
      "Epoch 26/450\n",
      "614/614 [==============================] - 0s 237us/step - loss: 0.2889 - acc: 0.8941\n",
      "Epoch 27/450\n",
      "614/614 [==============================] - 0s 223us/step - loss: 0.2866 - acc: 0.8909\n",
      "Epoch 28/450\n",
      "614/614 [==============================] - 0s 231us/step - loss: 0.2822 - acc: 0.8974\n",
      "Epoch 29/450\n",
      "614/614 [==============================] - 0s 240us/step - loss: 0.2805 - acc: 0.8844\n",
      "Epoch 30/450\n",
      "614/614 [==============================] - 0s 234us/step - loss: 0.2803 - acc: 0.8958\n",
      "Epoch 31/450\n",
      "614/614 [==============================] - 0s 235us/step - loss: 0.2769 - acc: 0.8893\n",
      "Epoch 32/450\n",
      "614/614 [==============================] - 0s 220us/step - loss: 0.2752 - acc: 0.8974\n",
      "Epoch 33/450\n",
      "614/614 [==============================] - 0s 232us/step - loss: 0.2722 - acc: 0.8958\n",
      "Epoch 34/450\n",
      "614/614 [==============================] - 0s 228us/step - loss: 0.2746 - acc: 0.9007\n",
      "Epoch 35/450\n",
      "614/614 [==============================] - 0s 242us/step - loss: 0.2700 - acc: 0.8958\n",
      "Epoch 36/450\n",
      "614/614 [==============================] - 0s 232us/step - loss: 0.2697 - acc: 0.8990\n",
      "Epoch 37/450\n",
      "614/614 [==============================] - 0s 230us/step - loss: 0.2668 - acc: 0.8958\n",
      "Epoch 38/450\n",
      "614/614 [==============================] - 0s 242us/step - loss: 0.2647 - acc: 0.9039\n",
      "Epoch 39/450\n",
      "614/614 [==============================] - 0s 239us/step - loss: 0.2663 - acc: 0.8990\n",
      "Epoch 40/450\n",
      "614/614 [==============================] - 0s 240us/step - loss: 0.2639 - acc: 0.8909\n",
      "Epoch 41/450\n",
      "614/614 [==============================] - 0s 233us/step - loss: 0.2646 - acc: 0.8974\n",
      "Epoch 42/450\n",
      "614/614 [==============================] - 0s 241us/step - loss: 0.2615 - acc: 0.9039\n",
      "Epoch 43/450\n",
      "614/614 [==============================] - 0s 225us/step - loss: 0.2554 - acc: 0.8990\n",
      "Epoch 44/450\n",
      "614/614 [==============================] - 0s 218us/step - loss: 0.2554 - acc: 0.8958\n",
      "Epoch 45/450\n",
      "614/614 [==============================] - 0s 217us/step - loss: 0.2573 - acc: 0.8990\n",
      "Epoch 46/450\n",
      "614/614 [==============================] - 0s 207us/step - loss: 0.2590 - acc: 0.9055\n",
      "Epoch 47/450\n",
      "614/614 [==============================] - 0s 218us/step - loss: 0.2569 - acc: 0.8990\n",
      "Epoch 48/450\n",
      "614/614 [==============================] - 0s 222us/step - loss: 0.2580 - acc: 0.9023\n",
      "Epoch 49/450\n",
      "614/614 [==============================] - 0s 216us/step - loss: 0.2558 - acc: 0.8974\n",
      "Epoch 50/450\n",
      "614/614 [==============================] - 0s 207us/step - loss: 0.2551 - acc: 0.9023\n",
      "Epoch 51/450\n",
      "614/614 [==============================] - 0s 210us/step - loss: 0.2532 - acc: 0.9023\n",
      "Epoch 52/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.2539 - acc: 0.8941\n",
      "Epoch 53/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.2527 - acc: 0.9055\n",
      "Epoch 54/450\n",
      "614/614 [==============================] - 0s 208us/step - loss: 0.2525 - acc: 0.9039\n",
      "Epoch 55/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.2514 - acc: 0.9055\n",
      "Epoch 56/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.2493 - acc: 0.8974\n",
      "Epoch 57/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.2490 - acc: 0.8990\n",
      "Epoch 58/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.2504 - acc: 0.8958\n",
      "Epoch 59/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.2454 - acc: 0.9072\n",
      "Epoch 60/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2473 - acc: 0.9104\n",
      "Epoch 61/450\n",
      "614/614 [==============================] - 0s 223us/step - loss: 0.2492 - acc: 0.8941\n",
      "Epoch 62/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.2455 - acc: 0.9121\n",
      "Epoch 63/450\n",
      "614/614 [==============================] - 0s 214us/step - loss: 0.2458 - acc: 0.8974\n",
      "Epoch 64/450\n",
      "614/614 [==============================] - 0s 166us/step - loss: 0.2445 - acc: 0.8974\n",
      "Epoch 65/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.2457 - acc: 0.9088\n",
      "Epoch 66/450\n",
      "614/614 [==============================] - 0s 210us/step - loss: 0.2438 - acc: 0.9104\n",
      "Epoch 67/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.2425 - acc: 0.9039\n",
      "Epoch 68/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.2437 - acc: 0.9104\n",
      "Epoch 69/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.2415 - acc: 0.9104\n",
      "Epoch 70/450\n",
      "614/614 [==============================] - 0s 206us/step - loss: 0.2393 - acc: 0.9007\n",
      "Epoch 71/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.2387 - acc: 0.9055\n",
      "Epoch 72/450\n",
      "614/614 [==============================] - 0s 206us/step - loss: 0.2406 - acc: 0.9137\n",
      "Epoch 73/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2398 - acc: 0.9104\n",
      "Epoch 74/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.2390 - acc: 0.9104\n",
      "Epoch 75/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2346 - acc: 0.9072\n",
      "Epoch 76/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.2407 - acc: 0.9088\n",
      "Epoch 77/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.2384 - acc: 0.9088\n",
      "Epoch 78/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.2361 - acc: 0.9072\n",
      "Epoch 79/450\n",
      "614/614 [==============================] - 0s 208us/step - loss: 0.2378 - acc: 0.9121\n",
      "Epoch 80/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.2347 - acc: 0.9104\n",
      "Epoch 81/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2341 - acc: 0.9121\n",
      "Epoch 82/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.2341 - acc: 0.9039\n",
      "Epoch 83/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.2353 - acc: 0.9055\n",
      "Epoch 84/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2335 - acc: 0.9104\n",
      "Epoch 85/450\n",
      "614/614 [==============================] - 0s 207us/step - loss: 0.2331 - acc: 0.9104\n",
      "Epoch 86/450\n",
      "614/614 [==============================] - 0s 209us/step - loss: 0.2315 - acc: 0.9088\n",
      "Epoch 87/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2342 - acc: 0.9072\n",
      "Epoch 88/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.2324 - acc: 0.9121\n",
      "Epoch 89/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.2320 - acc: 0.9055\n",
      "Epoch 90/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.2278 - acc: 0.9055\n",
      "Epoch 91/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.2317 - acc: 0.9088\n",
      "Epoch 92/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.2291 - acc: 0.9072\n",
      "Epoch 93/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.2277 - acc: 0.9072\n",
      "Epoch 94/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2269 - acc: 0.9088\n",
      "Epoch 95/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.2291 - acc: 0.9137\n",
      "Epoch 96/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2298 - acc: 0.9088\n",
      "Epoch 97/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2260 - acc: 0.9137\n",
      "Epoch 98/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.2303 - acc: 0.8990\n",
      "Epoch 99/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.2287 - acc: 0.9137\n",
      "Epoch 100/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.2248 - acc: 0.9088\n",
      "Epoch 101/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2262 - acc: 0.9121\n",
      "Epoch 102/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2280 - acc: 0.9137\n",
      "Epoch 103/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2254 - acc: 0.9121\n",
      "Epoch 104/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.2235 - acc: 0.9153\n",
      "Epoch 105/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2244 - acc: 0.9169\n",
      "Epoch 106/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.2228 - acc: 0.9072\n",
      "Epoch 107/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.2217 - acc: 0.9088\n",
      "Epoch 108/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2242 - acc: 0.9202\n",
      "Epoch 109/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2226 - acc: 0.9088\n",
      "Epoch 110/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.2205 - acc: 0.9088\n",
      "Epoch 111/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.2207 - acc: 0.9088\n",
      "Epoch 112/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.2218 - acc: 0.9169\n",
      "Epoch 113/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.2219 - acc: 0.9153\n",
      "Epoch 114/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.2197 - acc: 0.9137\n",
      "Epoch 115/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.2217 - acc: 0.9153\n",
      "Epoch 116/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.2189 - acc: 0.9153\n",
      "Epoch 117/450\n",
      "614/614 [==============================] - 0s 210us/step - loss: 0.2195 - acc: 0.9153\n",
      "Epoch 118/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2193 - acc: 0.9169\n",
      "Epoch 119/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2194 - acc: 0.9088\n",
      "Epoch 120/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.2178 - acc: 0.9121\n",
      "Epoch 121/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.2145 - acc: 0.9072\n",
      "Epoch 122/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.2127 - acc: 0.9121\n",
      "Epoch 123/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.2152 - acc: 0.9104\n",
      "Epoch 124/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.2150 - acc: 0.9088\n",
      "Epoch 125/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.2113 - acc: 0.9153\n",
      "Epoch 126/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.2094 - acc: 0.9153\n",
      "Epoch 127/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.2135 - acc: 0.9202\n",
      "Epoch 128/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2106 - acc: 0.9202\n",
      "Epoch 129/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.2087 - acc: 0.9104\n",
      "Epoch 130/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.2080 - acc: 0.9169\n",
      "Epoch 131/450\n",
      "614/614 [==============================] - 0s 211us/step - loss: 0.2027 - acc: 0.9169\n",
      "Epoch 132/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.2046 - acc: 0.9104\n",
      "Epoch 133/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2037 - acc: 0.9202\n",
      "Epoch 134/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.2016 - acc: 0.9169\n",
      "Epoch 135/450\n",
      "614/614 [==============================] - 0s 183us/step - loss: 0.2003 - acc: 0.9169\n",
      "Epoch 136/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.2014 - acc: 0.9186\n",
      "Epoch 137/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.2003 - acc: 0.9202\n",
      "Epoch 138/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.2013 - acc: 0.9218\n",
      "Epoch 139/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.1967 - acc: 0.9202\n",
      "Epoch 140/450\n",
      "614/614 [==============================] - 0s 175us/step - loss: 0.1981 - acc: 0.9202\n",
      "Epoch 141/450\n",
      "614/614 [==============================] - 0s 181us/step - loss: 0.1970 - acc: 0.9169\n",
      "Epoch 142/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1955 - acc: 0.9169\n",
      "Epoch 143/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1943 - acc: 0.9235\n",
      "Epoch 144/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1926 - acc: 0.9235\n",
      "Epoch 145/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1937 - acc: 0.9202\n",
      "Epoch 146/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1912 - acc: 0.9153\n",
      "Epoch 147/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.1941 - acc: 0.9153\n",
      "Epoch 148/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1911 - acc: 0.9251\n",
      "Epoch 149/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1901 - acc: 0.9202\n",
      "Epoch 150/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1924 - acc: 0.9202\n",
      "Epoch 151/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1898 - acc: 0.9218\n",
      "Epoch 152/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1893 - acc: 0.9267\n",
      "Epoch 153/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1872 - acc: 0.9235\n",
      "Epoch 154/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1882 - acc: 0.9202\n",
      "Epoch 155/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1869 - acc: 0.9218\n",
      "Epoch 156/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1867 - acc: 0.9267\n",
      "Epoch 157/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1868 - acc: 0.9218\n",
      "Epoch 158/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1789 - acc: 0.9267\n",
      "Epoch 159/450\n",
      "614/614 [==============================] - 0s 165us/step - loss: 0.1828 - acc: 0.9300\n",
      "Epoch 160/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1808 - acc: 0.9267\n",
      "Epoch 161/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1832 - acc: 0.9267\n",
      "Epoch 162/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1812 - acc: 0.9300\n",
      "Epoch 163/450\n",
      "614/614 [==============================] - 0s 209us/step - loss: 0.1787 - acc: 0.9283\n",
      "Epoch 164/450\n",
      "614/614 [==============================] - 0s 213us/step - loss: 0.1766 - acc: 0.9316\n",
      "Epoch 165/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1792 - acc: 0.9202\n",
      "Epoch 166/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1790 - acc: 0.9251\n",
      "Epoch 167/450\n",
      "614/614 [==============================] - 0s 173us/step - loss: 0.1768 - acc: 0.9349\n",
      "Epoch 168/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1767 - acc: 0.9251\n",
      "Epoch 169/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1695 - acc: 0.9251\n",
      "Epoch 170/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1735 - acc: 0.9300\n",
      "Epoch 171/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1731 - acc: 0.9332\n",
      "Epoch 172/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.1744 - acc: 0.9283\n",
      "Epoch 173/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1733 - acc: 0.9316\n",
      "Epoch 174/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1736 - acc: 0.9300\n",
      "Epoch 175/450\n",
      "614/614 [==============================] - 0s 181us/step - loss: 0.1694 - acc: 0.9283\n",
      "Epoch 176/450\n",
      "614/614 [==============================] - 0s 182us/step - loss: 0.1697 - acc: 0.9332\n",
      "Epoch 177/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1672 - acc: 0.9414\n",
      "Epoch 178/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1691 - acc: 0.9332\n",
      "Epoch 179/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1680 - acc: 0.9332\n",
      "Epoch 180/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1672 - acc: 0.9381\n",
      "Epoch 181/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1667 - acc: 0.9316\n",
      "Epoch 182/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1687 - acc: 0.9381\n",
      "Epoch 183/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1630 - acc: 0.9446\n",
      "Epoch 184/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1660 - acc: 0.9316\n",
      "Epoch 185/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1618 - acc: 0.9414\n",
      "Epoch 186/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1638 - acc: 0.9365\n",
      "Epoch 187/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1628 - acc: 0.9365\n",
      "Epoch 188/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1602 - acc: 0.9365\n",
      "Epoch 189/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1615 - acc: 0.9446\n",
      "Epoch 190/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1606 - acc: 0.9414\n",
      "Epoch 191/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1609 - acc: 0.9365\n",
      "Epoch 192/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1598 - acc: 0.9479\n",
      "Epoch 193/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1584 - acc: 0.9430\n",
      "Epoch 194/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1591 - acc: 0.9414\n",
      "Epoch 195/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1553 - acc: 0.9414\n",
      "Epoch 196/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1519 - acc: 0.9511\n",
      "Epoch 197/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.1568 - acc: 0.9365\n",
      "Epoch 198/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1575 - acc: 0.9463\n",
      "Epoch 199/450\n",
      "614/614 [==============================] - 0s 183us/step - loss: 0.1548 - acc: 0.9430\n",
      "Epoch 200/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1558 - acc: 0.9430\n",
      "Epoch 201/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1539 - acc: 0.9446\n",
      "Epoch 202/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1544 - acc: 0.9430\n",
      "Epoch 203/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1552 - acc: 0.9479\n",
      "Epoch 204/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1542 - acc: 0.9495\n",
      "Epoch 205/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1472 - acc: 0.9495\n",
      "Epoch 206/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1514 - acc: 0.9495\n",
      "Epoch 207/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1502 - acc: 0.9381\n",
      "Epoch 208/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1510 - acc: 0.9463\n",
      "Epoch 209/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1516 - acc: 0.9511\n",
      "Epoch 210/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1468 - acc: 0.9495\n",
      "Epoch 211/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1498 - acc: 0.9463\n",
      "Epoch 212/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1484 - acc: 0.9479\n",
      "Epoch 213/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1464 - acc: 0.9463\n",
      "Epoch 214/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1485 - acc: 0.9446\n",
      "Epoch 215/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1480 - acc: 0.9479\n",
      "Epoch 216/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1482 - acc: 0.9511\n",
      "Epoch 217/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.1453 - acc: 0.9544\n",
      "Epoch 218/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1482 - acc: 0.9479\n",
      "Epoch 219/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1448 - acc: 0.9560\n",
      "Epoch 220/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1456 - acc: 0.9446\n",
      "Epoch 221/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1471 - acc: 0.9511\n",
      "Epoch 222/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1440 - acc: 0.9560\n",
      "Epoch 223/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1457 - acc: 0.9560\n",
      "Epoch 224/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1430 - acc: 0.9544\n",
      "Epoch 225/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1412 - acc: 0.9463\n",
      "Epoch 226/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1412 - acc: 0.9511\n",
      "Epoch 227/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1440 - acc: 0.9511\n",
      "Epoch 228/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1412 - acc: 0.9528\n",
      "Epoch 229/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1444 - acc: 0.9463\n",
      "Epoch 230/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1389 - acc: 0.9560\n",
      "Epoch 231/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1414 - acc: 0.9544\n",
      "Epoch 232/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1421 - acc: 0.9495\n",
      "Epoch 233/450\n",
      "614/614 [==============================] - 0s 183us/step - loss: 0.1414 - acc: 0.9560\n",
      "Epoch 234/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1411 - acc: 0.9528\n",
      "Epoch 235/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1405 - acc: 0.9528\n",
      "Epoch 236/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1386 - acc: 0.9479\n",
      "Epoch 237/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1402 - acc: 0.9511\n",
      "Epoch 238/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1394 - acc: 0.9511\n",
      "Epoch 239/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1380 - acc: 0.9544\n",
      "Epoch 240/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1376 - acc: 0.9511\n",
      "Epoch 241/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1370 - acc: 0.9528\n",
      "Epoch 242/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1401 - acc: 0.9577\n",
      "Epoch 243/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1381 - acc: 0.9495\n",
      "Epoch 244/450\n",
      "614/614 [==============================] - 0s 183us/step - loss: 0.1388 - acc: 0.9544\n",
      "Epoch 245/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1378 - acc: 0.9495\n",
      "Epoch 246/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1359 - acc: 0.9511\n",
      "Epoch 247/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1386 - acc: 0.9544\n",
      "Epoch 248/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1356 - acc: 0.9577\n",
      "Epoch 249/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1332 - acc: 0.9544\n",
      "Epoch 250/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1330 - acc: 0.9544\n",
      "Epoch 251/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1316 - acc: 0.9544\n",
      "Epoch 252/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1291 - acc: 0.9495\n",
      "Epoch 253/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1338 - acc: 0.9593\n",
      "Epoch 254/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1357 - acc: 0.9495\n",
      "Epoch 255/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1355 - acc: 0.9560\n",
      "Epoch 256/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1354 - acc: 0.9511\n",
      "Epoch 257/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1340 - acc: 0.9463\n",
      "Epoch 258/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1352 - acc: 0.9593\n",
      "Epoch 259/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1318 - acc: 0.9528\n",
      "Epoch 260/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1345 - acc: 0.9593\n",
      "Epoch 261/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1297 - acc: 0.9577\n",
      "Epoch 262/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.1327 - acc: 0.9560\n",
      "Epoch 263/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1316 - acc: 0.9544\n",
      "Epoch 264/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1318 - acc: 0.9511\n",
      "Epoch 265/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1300 - acc: 0.9511\n",
      "Epoch 266/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1311 - acc: 0.9511\n",
      "Epoch 267/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1291 - acc: 0.9577\n",
      "Epoch 268/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.1290 - acc: 0.9593\n",
      "Epoch 269/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1313 - acc: 0.9511\n",
      "Epoch 270/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1301 - acc: 0.9528\n",
      "Epoch 271/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1297 - acc: 0.9577\n",
      "Epoch 272/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1280 - acc: 0.9528\n",
      "Epoch 273/450\n",
      "614/614 [==============================] - 0s 181us/step - loss: 0.1313 - acc: 0.9577\n",
      "Epoch 274/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1284 - acc: 0.9544\n",
      "Epoch 275/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1294 - acc: 0.9544\n",
      "Epoch 276/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1275 - acc: 0.9528\n",
      "Epoch 277/450\n",
      "614/614 [==============================] - 0s 179us/step - loss: 0.1285 - acc: 0.9479\n",
      "Epoch 278/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1289 - acc: 0.9528\n",
      "Epoch 279/450\n",
      "614/614 [==============================] - 0s 174us/step - loss: 0.1256 - acc: 0.9528\n",
      "Epoch 280/450\n",
      "614/614 [==============================] - 0s 179us/step - loss: 0.1322 - acc: 0.9544\n",
      "Epoch 281/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1244 - acc: 0.9560\n",
      "Epoch 282/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1285 - acc: 0.9528\n",
      "Epoch 283/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1261 - acc: 0.9528\n",
      "Epoch 284/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.1247 - acc: 0.9511\n",
      "Epoch 285/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1243 - acc: 0.9577\n",
      "Epoch 286/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1231 - acc: 0.9544\n",
      "Epoch 287/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1280 - acc: 0.9560\n",
      "Epoch 288/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1245 - acc: 0.9577\n",
      "Epoch 289/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1248 - acc: 0.9511\n",
      "Epoch 290/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1236 - acc: 0.9560\n",
      "Epoch 291/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1253 - acc: 0.9511\n",
      "Epoch 292/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.1248 - acc: 0.9544\n",
      "Epoch 293/450\n",
      "614/614 [==============================] - 0s 206us/step - loss: 0.1251 - acc: 0.9528\n",
      "Epoch 294/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1221 - acc: 0.9511\n",
      "Epoch 295/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1198 - acc: 0.9495\n",
      "Epoch 296/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1220 - acc: 0.9593\n",
      "Epoch 297/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1196 - acc: 0.9544\n",
      "Epoch 298/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1252 - acc: 0.9544\n",
      "Epoch 299/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1244 - acc: 0.9511\n",
      "Epoch 300/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1238 - acc: 0.9511\n",
      "Epoch 301/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.1236 - acc: 0.9560\n",
      "Epoch 302/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1232 - acc: 0.9544\n",
      "Epoch 303/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.1251 - acc: 0.9593\n",
      "Epoch 304/450\n",
      "614/614 [==============================] - 0s 211us/step - loss: 0.1202 - acc: 0.9528\n",
      "Epoch 305/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1222 - acc: 0.9511\n",
      "Epoch 306/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1212 - acc: 0.9544\n",
      "Epoch 307/450\n",
      "614/614 [==============================] - 0s 211us/step - loss: 0.1206 - acc: 0.9544\n",
      "Epoch 308/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1204 - acc: 0.9560\n",
      "Epoch 309/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1209 - acc: 0.9544\n",
      "Epoch 310/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1225 - acc: 0.9528\n",
      "Epoch 311/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1172 - acc: 0.9511\n",
      "Epoch 312/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1186 - acc: 0.9560\n",
      "Epoch 313/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1171 - acc: 0.9528\n",
      "Epoch 314/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1181 - acc: 0.9495\n",
      "Epoch 315/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.1159 - acc: 0.9577\n",
      "Epoch 316/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1184 - acc: 0.9479\n",
      "Epoch 317/450\n",
      "614/614 [==============================] - 0s 185us/step - loss: 0.1171 - acc: 0.9577\n",
      "Epoch 318/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1192 - acc: 0.9544\n",
      "Epoch 319/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1137 - acc: 0.9560\n",
      "Epoch 320/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1186 - acc: 0.9544\n",
      "Epoch 321/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1172 - acc: 0.9577\n",
      "Epoch 322/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1169 - acc: 0.9544\n",
      "Epoch 323/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1151 - acc: 0.9528\n",
      "Epoch 324/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.1179 - acc: 0.9495\n",
      "Epoch 325/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1130 - acc: 0.9544\n",
      "Epoch 326/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1154 - acc: 0.9528\n",
      "Epoch 327/450\n",
      "614/614 [==============================] - 0s 241us/step - loss: 0.1152 - acc: 0.9593\n",
      "Epoch 328/450\n",
      "614/614 [==============================] - 0s 211us/step - loss: 0.1159 - acc: 0.9528\n",
      "Epoch 329/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.1081 - acc: 0.9577\n",
      "Epoch 330/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1192 - acc: 0.9577\n",
      "Epoch 331/450\n",
      "614/614 [==============================] - 0s 205us/step - loss: 0.1161 - acc: 0.9528\n",
      "Epoch 332/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1164 - acc: 0.9560\n",
      "Epoch 333/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1161 - acc: 0.9560\n",
      "Epoch 334/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1134 - acc: 0.9560\n",
      "Epoch 335/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.1137 - acc: 0.9528\n",
      "Epoch 336/450\n",
      "614/614 [==============================] - 0s 182us/step - loss: 0.1153 - acc: 0.9544\n",
      "Epoch 337/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.1097 - acc: 0.9544\n",
      "Epoch 338/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.1156 - acc: 0.9577\n",
      "Epoch 339/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1124 - acc: 0.9560\n",
      "Epoch 340/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1114 - acc: 0.9544\n",
      "Epoch 341/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1129 - acc: 0.9577\n",
      "Epoch 342/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1135 - acc: 0.9511\n",
      "Epoch 343/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.1131 - acc: 0.9560\n",
      "Epoch 344/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1114 - acc: 0.9593\n",
      "Epoch 345/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1140 - acc: 0.9544\n",
      "Epoch 346/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1100 - acc: 0.9544\n",
      "Epoch 347/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1115 - acc: 0.9593\n",
      "Epoch 348/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1134 - acc: 0.9593\n",
      "Epoch 349/450\n",
      "614/614 [==============================] - 0s 206us/step - loss: 0.1094 - acc: 0.9609\n",
      "Epoch 350/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1087 - acc: 0.9528\n",
      "Epoch 351/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1111 - acc: 0.9560\n",
      "Epoch 352/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.1116 - acc: 0.9560\n",
      "Epoch 353/450\n",
      "614/614 [==============================] - 0s 203us/step - loss: 0.1094 - acc: 0.9560\n",
      "Epoch 354/450\n",
      "614/614 [==============================] - 0s 225us/step - loss: 0.1040 - acc: 0.9625\n",
      "Epoch 355/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.1058 - acc: 0.9544\n",
      "Epoch 356/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1097 - acc: 0.9577\n",
      "Epoch 357/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1074 - acc: 0.9544\n",
      "Epoch 358/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1092 - acc: 0.9577\n",
      "Epoch 359/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1069 - acc: 0.9560\n",
      "Epoch 360/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1062 - acc: 0.9511\n",
      "Epoch 361/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1088 - acc: 0.9609\n",
      "Epoch 362/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1109 - acc: 0.9593\n",
      "Epoch 363/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.1127 - acc: 0.9495\n",
      "Epoch 364/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1066 - acc: 0.9544\n",
      "Epoch 365/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1090 - acc: 0.9577\n",
      "Epoch 366/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1039 - acc: 0.9560\n",
      "Epoch 367/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.1049 - acc: 0.9577\n",
      "Epoch 368/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1053 - acc: 0.9642\n",
      "Epoch 369/450\n",
      "614/614 [==============================] - 0s 208us/step - loss: 0.1062 - acc: 0.9560\n",
      "Epoch 370/450\n",
      "614/614 [==============================] - 0s 211us/step - loss: 0.1060 - acc: 0.9625\n",
      "Epoch 371/450\n",
      "614/614 [==============================] - 0s 214us/step - loss: 0.1035 - acc: 0.9625\n",
      "Epoch 372/450\n",
      "614/614 [==============================] - 0s 231us/step - loss: 0.1054 - acc: 0.9593\n",
      "Epoch 373/450\n",
      "614/614 [==============================] - 0s 216us/step - loss: 0.1050 - acc: 0.9544\n",
      "Epoch 374/450\n",
      "614/614 [==============================] - 0s 213us/step - loss: 0.1072 - acc: 0.9560\n",
      "Epoch 375/450\n",
      "614/614 [==============================] - 0s 206us/step - loss: 0.1052 - acc: 0.9625\n",
      "Epoch 376/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.1038 - acc: 0.9593\n",
      "Epoch 377/450\n",
      "614/614 [==============================] - 0s 269us/step - loss: 0.1022 - acc: 0.9593\n",
      "Epoch 378/450\n",
      "614/614 [==============================] - 0s 212us/step - loss: 0.1056 - acc: 0.9593\n",
      "Epoch 379/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.1016 - acc: 0.9577\n",
      "Epoch 380/450\n",
      "614/614 [==============================] - 0s 217us/step - loss: 0.0998 - acc: 0.9642\n",
      "Epoch 381/450\n",
      "614/614 [==============================] - 0s 263us/step - loss: 0.1065 - acc: 0.9511\n",
      "Epoch 382/450\n",
      "614/614 [==============================] - 0s 252us/step - loss: 0.1003 - acc: 0.9625\n",
      "Epoch 383/450\n",
      "614/614 [==============================] - 0s 223us/step - loss: 0.1015 - acc: 0.9593\n",
      "Epoch 384/450\n",
      "614/614 [==============================] - 0s 202us/step - loss: 0.1020 - acc: 0.9593\n",
      "Epoch 385/450\n",
      "614/614 [==============================] - 0s 207us/step - loss: 0.1040 - acc: 0.9577\n",
      "Epoch 386/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.1036 - acc: 0.9593\n",
      "Epoch 387/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.1013 - acc: 0.9593\n",
      "Epoch 388/450\n",
      "614/614 [==============================] - 0s 182us/step - loss: 0.1033 - acc: 0.9593\n",
      "Epoch 389/450\n",
      "614/614 [==============================] - 0s 243us/step - loss: 0.1018 - acc: 0.9658\n",
      "Epoch 390/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.0997 - acc: 0.9577\n",
      "Epoch 391/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.0963 - acc: 0.9625\n",
      "Epoch 392/450\n",
      "614/614 [==============================] - 0s 303us/step - loss: 0.0964 - acc: 0.9674\n",
      "Epoch 393/450\n",
      "614/614 [==============================] - 0s 272us/step - loss: 0.1047 - acc: 0.9609\n",
      "Epoch 394/450\n",
      "614/614 [==============================] - 0s 197us/step - loss: 0.1020 - acc: 0.9577\n",
      "Epoch 395/450\n",
      "614/614 [==============================] - 0s 208us/step - loss: 0.1006 - acc: 0.9593\n",
      "Epoch 396/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.0940 - acc: 0.9625\n",
      "Epoch 397/450\n",
      "614/614 [==============================] - 0s 192us/step - loss: 0.1012 - acc: 0.9593\n",
      "Epoch 398/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.1021 - acc: 0.9609\n",
      "Epoch 399/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.1004 - acc: 0.9625\n",
      "Epoch 400/450\n",
      "614/614 [==============================] - 0s 284us/step - loss: 0.0989 - acc: 0.9642\n",
      "Epoch 401/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.0920 - acc: 0.9674\n",
      "Epoch 402/450\n",
      "614/614 [==============================] - 0s 166us/step - loss: 0.0974 - acc: 0.9593\n",
      "Epoch 403/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.0982 - acc: 0.9625\n",
      "Epoch 404/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.1029 - acc: 0.9593\n",
      "Epoch 405/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.0985 - acc: 0.9625\n",
      "Epoch 406/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0962 - acc: 0.9625\n",
      "Epoch 407/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.0957 - acc: 0.9642\n",
      "Epoch 408/450\n",
      "614/614 [==============================] - 0s 225us/step - loss: 0.0956 - acc: 0.9642\n",
      "Epoch 409/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.0962 - acc: 0.9642\n",
      "Epoch 410/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.0939 - acc: 0.9593\n",
      "Epoch 411/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.0934 - acc: 0.9642\n",
      "Epoch 412/450\n",
      "614/614 [==============================] - 0s 215us/step - loss: 0.1008 - acc: 0.9609\n",
      "Epoch 413/450\n",
      "614/614 [==============================] - 0s 201us/step - loss: 0.0947 - acc: 0.9674\n",
      "Epoch 414/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0950 - acc: 0.9544\n",
      "Epoch 415/450\n",
      "614/614 [==============================] - 0s 243us/step - loss: 0.0936 - acc: 0.9544\n",
      "Epoch 416/450\n",
      "614/614 [==============================] - 0s 279us/step - loss: 0.0993 - acc: 0.9560\n",
      "Epoch 417/450\n",
      "614/614 [==============================] - 0s 223us/step - loss: 0.0955 - acc: 0.9625\n",
      "Epoch 418/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.0978 - acc: 0.9642\n",
      "Epoch 419/450\n",
      "614/614 [==============================] - 0s 189us/step - loss: 0.0944 - acc: 0.9609\n",
      "Epoch 420/450\n",
      "614/614 [==============================] - 0s 204us/step - loss: 0.0915 - acc: 0.9609\n",
      "Epoch 421/450\n",
      "614/614 [==============================] - 0s 196us/step - loss: 0.0898 - acc: 0.9577\n",
      "Epoch 422/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.0951 - acc: 0.9642\n",
      "Epoch 423/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0935 - acc: 0.9658\n",
      "Epoch 424/450\n",
      "614/614 [==============================] - 0s 218us/step - loss: 0.0952 - acc: 0.9593\n",
      "Epoch 425/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0932 - acc: 0.9609\n",
      "Epoch 426/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.0945 - acc: 0.9658\n",
      "Epoch 427/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.0915 - acc: 0.9658\n",
      "Epoch 428/450\n",
      "614/614 [==============================] - 0s 184us/step - loss: 0.0915 - acc: 0.9658\n",
      "Epoch 429/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.0914 - acc: 0.9642\n",
      "Epoch 430/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0951 - acc: 0.9642\n",
      "Epoch 431/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.0910 - acc: 0.9625\n",
      "Epoch 432/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.0935 - acc: 0.9625\n",
      "Epoch 433/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.0879 - acc: 0.9642\n",
      "Epoch 434/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0891 - acc: 0.9658\n",
      "Epoch 435/450\n",
      "614/614 [==============================] - 0s 199us/step - loss: 0.0891 - acc: 0.9691\n",
      "Epoch 436/450\n",
      "614/614 [==============================] - 0s 187us/step - loss: 0.0889 - acc: 0.9625\n",
      "Epoch 437/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0850 - acc: 0.9674\n",
      "Epoch 438/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.0866 - acc: 0.9642\n",
      "Epoch 439/450\n",
      "614/614 [==============================] - 0s 194us/step - loss: 0.0910 - acc: 0.9625\n",
      "Epoch 440/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0899 - acc: 0.9691\n",
      "Epoch 441/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0854 - acc: 0.9642\n",
      "Epoch 442/450\n",
      "614/614 [==============================] - 0s 198us/step - loss: 0.0890 - acc: 0.9658\n",
      "Epoch 443/450\n",
      "614/614 [==============================] - 0s 188us/step - loss: 0.0896 - acc: 0.9658\n",
      "Epoch 444/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0851 - acc: 0.9723\n",
      "Epoch 445/450\n",
      "614/614 [==============================] - 0s 191us/step - loss: 0.0893 - acc: 0.9609\n",
      "Epoch 446/450\n",
      "614/614 [==============================] - 0s 190us/step - loss: 0.0863 - acc: 0.9674\n",
      "Epoch 447/450\n",
      "614/614 [==============================] - 0s 200us/step - loss: 0.0869 - acc: 0.9674\n",
      "Epoch 448/450\n",
      "614/614 [==============================] - 0s 186us/step - loss: 0.0884 - acc: 0.9691\n",
      "Epoch 449/450\n",
      "614/614 [==============================] - 0s 193us/step - loss: 0.0866 - acc: 0.9642\n",
      "Epoch 450/450\n",
      "614/614 [==============================] - 0s 195us/step - loss: 0.0885 - acc: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5058c9a20>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, Y_train, epochs=450, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 3ms/step\n",
      "\n",
      "acc: 96.75%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pridict the model\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After tuining the parameters of the model like batch size ,epoch(450)and input , our accuracy was approximately coming as 96%. It can very in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
